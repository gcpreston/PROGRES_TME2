{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294988a5",
   "metadata": {},
   "source": [
    "# PROGRES - TME2\n",
    "\n",
    "Fabien Mathieu - fabien.mathieu@normalesup.org\n",
    "\n",
    "SÃ©bastien Tixeuil - Sebastien.Tixeuil@lip6.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9046b32",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "- Star exercises (indicated by *) should only be done if all other exercises have been completed. You \n",
    "don't have to do them if you do not want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b0fc5",
   "metadata": {},
   "source": [
    "# Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f73082",
   "metadata": {},
   "source": [
    "1. Cite your sources\n",
    "2. One file to rule them all\n",
    "3. Explain\n",
    "4. Execute your code\n",
    "\n",
    "\n",
    "https://github.com/balouf/progres/blob/main/rules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a42a0",
   "metadata": {},
   "source": [
    "# Exercice 1 - Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777c51e",
   "metadata": {},
   "source": [
    "Consider the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd42208b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.166734Z",
     "start_time": "2024-10-11T07:03:31.162693Z"
    }
   },
   "outputs": [],
   "source": [
    "L = ['marie.Dupond@gmail.com', 'lucie.Durand@wanadoo.fr',\n",
    "'Sophie.Parmentier @@ gmail.com', 'franck.Dupres.gmail.com',\n",
    "'pierre.Martin@lip6 .fr ',' eric.Deschamps@gmail.com ']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522f37b",
   "metadata": {},
   "source": [
    "- Which of these entries are valid?\n",
    "- Use regular expressions to identify valid *gmail* addresses and display them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031d815",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f803bc",
   "metadata": {},
   "source": [
    "The valid entries are `'marie.Dupond@gmail.com'`, `' eric.Deschamps@gmail.com '`. We consider otherwise valid strings which are whitespace-padded to also be valid, as stripping is a simple operation, and this lends itself to a better user experience (if the user doesn't realize there is an invisible space, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58857cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.182237Z",
     "start_time": "2024-10-11T07:03:31.167743Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import functools\n",
    "from typing import List\n",
    "\n",
    "GMAIL_RE = re.compile(r'^\\s*([0-9A-Za-z_.]+@gmail.com)\\s*')\n",
    "\n",
    "def _true_gmail_reducer(accumulator: List[str], test_address: str) -> bool:\n",
    "    gmail_match = GMAIL_RE.match(test_address)\n",
    "    if not gmail_match: return accumulator\n",
    "    address = gmail_match.group(1)\n",
    "    return accumulator + [address]\n",
    "\n",
    "def true_gmail(mail_list: List[str]) -> List[str]:\n",
    "    return functools.reduce(_true_gmail_reducer, mail_list, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2496b3",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The `true_gmail` transforms a list of strings to a list of found, whitespace-stripped, gmail addresses. Because values of the output list may be transformed from those of the input list, a `reduce` is used in place of a `filter`. \n",
    "\n",
    "The reducer implements the logic. It tests against a gmail regex and implements two cases:\n",
    "1. If there is no match, throw out the address by returning the unchanged accumulator\n",
    "2. Otherwise, continue to the next iteration with the desired portion of the address, by returning the accumulator with the address portion appended\n",
    "\n",
    "Note that `+` is used for list extension rather than `.append`. This is to prevent any unexpected behavior that could come from mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "816fd798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.210273Z",
     "start_time": "2024-10-11T07:03:31.196497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marie.Dupond@gmail.com', 'eric.Deschamps@gmail.com']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_gmail(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375b8f3",
   "metadata": {},
   "source": [
    "- Use regular expressions to check if a string ends with a number. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c3615",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13acfb67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.221802Z",
     "start_time": "2024-10-11T07:03:31.211282Z"
    }
   },
   "outputs": [],
   "source": [
    "def ends_with_number(txt: str) -> bool:\n",
    "    return bool(re.match(r'^.*\\d$', txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef169126",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "`ends_with_number` checks for a match of the given parameter against the regular expression `^.*\\d$`. The regular expression could be worded in English as: \"match anything from the beginning of the string, then match a number followed by the string end\".\n",
    "\n",
    "`re.match` returns a `Match` object if a match is present, and `None` otherwise, but `ends_with_number` wants to return a boolean indicating yes or no. The result of `re.match` is transformed to the desired output by simply being passed to `bool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9da640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.246076Z",
     "start_time": "2024-10-11T07:03:31.235381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ends_with_number('to42to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa5f293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.257782Z",
     "start_time": "2024-10-11T07:03:31.248084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ends_with_number('to42to666')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bde5d7",
   "metadata": {},
   "source": [
    "- Use regular expressions to remove problematic zeros from an IPv4 address expressed as a \n",
    "string. (example: \"216.08.094.196\" should become \"216.8.94.196\", but \"216.80.140.196\" \n",
    "should remain \"216.80.140.196\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f606b5",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d2f77ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.271323Z",
     "start_time": "2024-10-11T07:03:31.261798Z"
    }
   },
   "outputs": [],
   "source": [
    "IPV4_FIELD_RE = re.compile(r'0*(\\d{1,3})')\n",
    "\n",
    "def normalize_ip(txt):\n",
    "    return '.'.join(IPV4_FIELD_RE.findall(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f5056",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "`normalize_ip` uses a regular expression to match the desired substring for each sequence within an IPv4 address. The list of desired sequences is taken using `.findall`, which is then re-formatted to an IPv4 string using `'.'.join`. \n",
    "\n",
    "The regular expression used is `0*(\\d{1,3})`. There are two parts to this expression:\n",
    "1. `0*` matches 0 or more of the character `0`, at the beginning of the sequence, outside the capture group\n",
    "2. `(\\d{1,3})` matches 1-3 digits in a row for a sequence, and puts them in a capture group\n",
    "\n",
    "The first part enables excluding leading `0`s from the capture group, while not requiring leading `0`s to match. The second part matching at least 1 digit enables capturing a `0` if it is the actual value of the sequence. e.g: The edge case `'000'` matches only the last `0` within the capture group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088d349e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.295246Z",
     "start_time": "2024-10-11T07:03:31.284633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'216.0.94.196'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_ip(\"216.0.094.196\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57c2cb4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.309262Z",
     "start_time": "2024-10-11T07:03:31.296255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'216.8.94.196'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_ip(\"216.08.094.196\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b6be3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.350525Z",
     "start_time": "2024-10-11T07:03:31.313271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'216.80.140.196'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_ip(\"216.80.140.196\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ac9d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.0.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_ip(\"000.00.0.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defac980",
   "metadata": {},
   "source": [
    "- Use regular expressions to transform a date from MM-DD-YYYY format to DD-MM-YYYY \n",
    "format. (example \"11-06-2020\" should become \"06-11-2020\"). Optionally*, do the same thing using the `datetime` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c7974",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01892f64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.360858Z",
     "start_time": "2024-10-11T07:03:31.356546Z"
    }
   },
   "outputs": [],
   "source": [
    "DATE_RE = re.compile(r'^(\\d{2})-(\\d{2})-(\\d{4})$')\n",
    "\n",
    "def switch_md(txt: str) -> str:\n",
    "    mm, dd, yyyy = DATE_RE.match(txt).groups()\n",
    "    return '-'.join([dd, mm, yyyy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c93b73",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "`switch_md` uses a regex to match a full date string and grab groups of each section, then re-orders and re-joins them to the desired format.\n",
    "\n",
    "Note that it is assumed the `txt` parameter matches this format, and does not define behavior for when this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f731cbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.399470Z",
     "start_time": "2024-10-11T07:03:31.386752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'06-11-2020'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_md(\"11-06-2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1a107",
   "metadata": {},
   "source": [
    "# Exercice 2 - Analyze XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a77fec",
   "metadata": {},
   "source": [
    "- Write a Python code that retrieves the content of the page at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a640a342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.410291Z",
     "start_time": "2024-10-11T07:03:31.400475Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.w3schools.com/xml/cd_catalog.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2670c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "s = Session()\n",
    "r = s.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ff873",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "To retrieve the URL content, `Sessions.get` is used, to give the option to keep cookies and re-use a TCP connection if we were making multiple requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371f1f2",
   "metadata": {},
   "source": [
    "- Look at the text content and load as xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8c5d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<CATALOG>\n",
      "  <CD>\n",
      "    <TITLE>Empire Burlesque</TITLE>\n",
      "    <ARTIST>Bob Dylan</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Columbia</COMPANY>\n",
      "    <PRICE>10.90</PRICE>\n",
      "    <YEAR>1985</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Hide your heart</TITLE>\n",
      "    <ARTIST>Bonnie Tyler</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>CBS Records</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1988</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Greatest Hits</TITLE>\n",
      "    <ARTIST>Dolly Parton</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>RCA</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1982</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Still got the blues</TITLE>\n",
      "    <ARTIST>Gary Moore</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Virgin records</COMPANY>\n",
      "    <PRICE>10.20</PRICE>\n",
      "    <YEAR>1990</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Eros</TITLE>\n",
      "    <ARTIST>Eros Ramazzotti</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>BMG</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1997</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>One night only</TITLE>\n",
      "    <ARTIST>Bee Gees</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Polydor</COMPANY>\n",
      "    <PRICE>10.90</PRICE>\n",
      "    <YEAR>1998</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Sylvias Mother</TITLE>\n",
      "    <ARTIST>Dr.Hook</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>CBS</COMPANY>\n",
      "    <PRICE>8.10</PRICE>\n",
      "    <YEAR>1973</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Maggie May</TITLE>\n",
      "    <ARTIST>Rod Stewart</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Pickwick</COMPANY>\n",
      "    <PRICE>8.50</PRICE>\n",
      "    <YEAR>1990</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Romanza</TITLE>\n",
      "    <ARTIST>Andrea Bocelli</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Polydor</COMPANY>\n",
      "    <PRICE>10.80</PRICE>\n",
      "    <YEAR>1996</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>When a man loves a woman</TITLE>\n",
      "    <ARTIST>Percy Sledge</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Atlantic</COMPANY>\n",
      "    <PRICE>8.70</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Black angel</TITLE>\n",
      "    <ARTIST>Savage Rose</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Mega</COMPANY>\n",
      "    <PRICE>10.90</PRICE>\n",
      "    <YEAR>1995</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>1999 Grammy Nominees</TITLE>\n",
      "    <ARTIST>Many</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Grammy</COMPANY>\n",
      "    <PRICE>10.20</PRICE>\n",
      "    <YEAR>1999</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>For the good times</TITLE>\n",
      "    <ARTIST>Kenny Rogers</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Mucik Master</COMPANY>\n",
      "    <PRICE>8.70</PRICE>\n",
      "    <YEAR>1995</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Big Willie style</TITLE>\n",
      "    <ARTIST>Will Smith</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Columbia</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1997</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Tupelo Honey</TITLE>\n",
      "    <ARTIST>Van Morrison</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Polydor</COMPANY>\n",
      "    <PRICE>8.20</PRICE>\n",
      "    <YEAR>1971</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Soulsville</TITLE>\n",
      "    <ARTIST>Jorn Hoel</ARTIST>\n",
      "    <COUNTRY>Norway</COUNTRY>\n",
      "    <COMPANY>WEA</COMPANY>\n",
      "    <PRICE>7.90</PRICE>\n",
      "    <YEAR>1996</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>The very best of</TITLE>\n",
      "    <ARTIST>Cat Stevens</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Island</COMPANY>\n",
      "    <PRICE>8.90</PRICE>\n",
      "    <YEAR>1990</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Stop</TITLE>\n",
      "    <ARTIST>Sam Brown</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>A and M</COMPANY>\n",
      "    <PRICE>8.90</PRICE>\n",
      "    <YEAR>1988</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Bridge of Spies</TITLE>\n",
      "    <ARTIST>T'Pau</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Siren</COMPANY>\n",
      "    <PRICE>7.90</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Private Dancer</TITLE>\n",
      "    <ARTIST>Tina Turner</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Capitol</COMPANY>\n",
      "    <PRICE>8.90</PRICE>\n",
      "    <YEAR>1983</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Midt om natten</TITLE>\n",
      "    <ARTIST>Kim Larsen</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Medley</COMPANY>\n",
      "    <PRICE>7.80</PRICE>\n",
      "    <YEAR>1983</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Pavarotti Gala Concert</TITLE>\n",
      "    <ARTIST>Luciano Pavarotti</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>DECCA</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1991</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>The dock of the bay</TITLE>\n",
      "    <ARTIST>Otis Redding</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Stax Records</COMPANY>\n",
      "    <PRICE>7.90</PRICE>\n",
      "    <YEAR>1968</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Picture book</TITLE>\n",
      "    <ARTIST>Simply Red</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Elektra</COMPANY>\n",
      "    <PRICE>7.20</PRICE>\n",
      "    <YEAR>1985</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Red</TITLE>\n",
      "    <ARTIST>The Communards</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>London</COMPANY>\n",
      "    <PRICE>7.80</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Unchain my heart</TITLE>\n",
      "    <ARTIST>Joe Cocker</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>EMI</COMPANY>\n",
      "    <PRICE>8.20</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "</CATALOG>\n",
      "\n",
      "Main tag: CATALOG; main attributes: {}\n"
     ]
    }
   ],
   "source": [
    "print(r.text)\n",
    "cds = ET.fromstring(r.text)\n",
    "print(f\"Main tag: {cds.tag}; main attributes: {cds.attrib}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb1cdc",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "To load the result as XML, `ElementTree.fromstring` is used, for simplicity's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d26eb",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ed90b",
   "metadata": {},
   "source": [
    "- Write a `display_cd` function that displays (i.e. `print`), for a CD: title, artist, country, company, year.\n",
    "- Display all CDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c1c3b4",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ee85351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.730469Z",
     "start_time": "2024-10-11T07:03:31.712761Z"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_cd(cd: ET) -> None:\n",
    "    properties = [f'{child.tag}: {child.text}' for child in cd]\n",
    "    print(', '.join(properties))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934cb66",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The chosen format for displaying a CD is to display all child tags and their text content, separated by commas. This is done by first creating a list of tags + values with the desired format, and then utilizing `.join` to easily intersperse commas, and printing the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94c7d5",
   "metadata": {},
   "source": [
    "- Display all 1980s CDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f548d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: Empire Burlesque, ARTIST: Bob Dylan, COUNTRY: USA, COMPANY: Columbia, PRICE: 10.90, YEAR: 1985\n",
      "TITLE: Hide your heart, ARTIST: Bonnie Tyler, COUNTRY: UK, COMPANY: CBS Records, PRICE: 9.90, YEAR: 1988\n",
      "TITLE: Greatest Hits, ARTIST: Dolly Parton, COUNTRY: USA, COMPANY: RCA, PRICE: 9.90, YEAR: 1982\n",
      "TITLE: Still got the blues, ARTIST: Gary Moore, COUNTRY: UK, COMPANY: Virgin records, PRICE: 10.20, YEAR: 1990\n",
      "TITLE: Eros, ARTIST: Eros Ramazzotti, COUNTRY: EU, COMPANY: BMG, PRICE: 9.90, YEAR: 1997\n",
      "TITLE: One night only, ARTIST: Bee Gees, COUNTRY: UK, COMPANY: Polydor, PRICE: 10.90, YEAR: 1998\n",
      "TITLE: Sylvias Mother, ARTIST: Dr.Hook, COUNTRY: UK, COMPANY: CBS, PRICE: 8.10, YEAR: 1973\n",
      "TITLE: Maggie May, ARTIST: Rod Stewart, COUNTRY: UK, COMPANY: Pickwick, PRICE: 8.50, YEAR: 1990\n",
      "TITLE: Romanza, ARTIST: Andrea Bocelli, COUNTRY: EU, COMPANY: Polydor, PRICE: 10.80, YEAR: 1996\n",
      "TITLE: When a man loves a woman, ARTIST: Percy Sledge, COUNTRY: USA, COMPANY: Atlantic, PRICE: 8.70, YEAR: 1987\n",
      "TITLE: Black angel, ARTIST: Savage Rose, COUNTRY: EU, COMPANY: Mega, PRICE: 10.90, YEAR: 1995\n",
      "TITLE: 1999 Grammy Nominees, ARTIST: Many, COUNTRY: USA, COMPANY: Grammy, PRICE: 10.20, YEAR: 1999\n",
      "TITLE: For the good times, ARTIST: Kenny Rogers, COUNTRY: UK, COMPANY: Mucik Master, PRICE: 8.70, YEAR: 1995\n",
      "TITLE: Big Willie style, ARTIST: Will Smith, COUNTRY: USA, COMPANY: Columbia, PRICE: 9.90, YEAR: 1997\n",
      "TITLE: Tupelo Honey, ARTIST: Van Morrison, COUNTRY: UK, COMPANY: Polydor, PRICE: 8.20, YEAR: 1971\n",
      "TITLE: Soulsville, ARTIST: Jorn Hoel, COUNTRY: Norway, COMPANY: WEA, PRICE: 7.90, YEAR: 1996\n",
      "TITLE: The very best of, ARTIST: Cat Stevens, COUNTRY: UK, COMPANY: Island, PRICE: 8.90, YEAR: 1990\n",
      "TITLE: Stop, ARTIST: Sam Brown, COUNTRY: UK, COMPANY: A and M, PRICE: 8.90, YEAR: 1988\n",
      "TITLE: Bridge of Spies, ARTIST: T'Pau, COUNTRY: UK, COMPANY: Siren, PRICE: 7.90, YEAR: 1987\n",
      "TITLE: Private Dancer, ARTIST: Tina Turner, COUNTRY: UK, COMPANY: Capitol, PRICE: 8.90, YEAR: 1983\n",
      "TITLE: Midt om natten, ARTIST: Kim Larsen, COUNTRY: EU, COMPANY: Medley, PRICE: 7.80, YEAR: 1983\n",
      "TITLE: Pavarotti Gala Concert, ARTIST: Luciano Pavarotti, COUNTRY: UK, COMPANY: DECCA, PRICE: 9.90, YEAR: 1991\n",
      "TITLE: The dock of the bay, ARTIST: Otis Redding, COUNTRY: USA, COMPANY: Stax Records, PRICE: 7.90, YEAR: 1968\n",
      "TITLE: Picture book, ARTIST: Simply Red, COUNTRY: EU, COMPANY: Elektra, PRICE: 7.20, YEAR: 1985\n",
      "TITLE: Red, ARTIST: The Communards, COUNTRY: UK, COMPANY: London, PRICE: 7.80, YEAR: 1987\n",
      "TITLE: Unchain my heart, ARTIST: Joe Cocker, COUNTRY: USA, COMPANY: EMI, PRICE: 8.20, YEAR: 1987\n"
     ]
    }
   ],
   "source": [
    "for cd in cds:\n",
    "  display_cd(cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73709331",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The root element has CDs as sub-elements. Since `display_cd` expects a single CD record, we iterate through the root and pass each child to `display_cd`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d207ba3",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395138d",
   "metadata": {},
   "source": [
    "- Display all British CDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d7efe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: Hide your heart, ARTIST: Bonnie Tyler, COUNTRY: UK, COMPANY: CBS Records, PRICE: 9.90, YEAR: 1988\n",
      "TITLE: Still got the blues, ARTIST: Gary Moore, COUNTRY: UK, COMPANY: Virgin records, PRICE: 10.20, YEAR: 1990\n",
      "TITLE: One night only, ARTIST: Bee Gees, COUNTRY: UK, COMPANY: Polydor, PRICE: 10.90, YEAR: 1998\n",
      "TITLE: Sylvias Mother, ARTIST: Dr.Hook, COUNTRY: UK, COMPANY: CBS, PRICE: 8.10, YEAR: 1973\n",
      "TITLE: Maggie May, ARTIST: Rod Stewart, COUNTRY: UK, COMPANY: Pickwick, PRICE: 8.50, YEAR: 1990\n",
      "TITLE: For the good times, ARTIST: Kenny Rogers, COUNTRY: UK, COMPANY: Mucik Master, PRICE: 8.70, YEAR: 1995\n",
      "TITLE: Tupelo Honey, ARTIST: Van Morrison, COUNTRY: UK, COMPANY: Polydor, PRICE: 8.20, YEAR: 1971\n",
      "TITLE: The very best of, ARTIST: Cat Stevens, COUNTRY: UK, COMPANY: Island, PRICE: 8.90, YEAR: 1990\n",
      "TITLE: Stop, ARTIST: Sam Brown, COUNTRY: UK, COMPANY: A and M, PRICE: 8.90, YEAR: 1988\n",
      "TITLE: Bridge of Spies, ARTIST: T'Pau, COUNTRY: UK, COMPANY: Siren, PRICE: 7.90, YEAR: 1987\n",
      "TITLE: Private Dancer, ARTIST: Tina Turner, COUNTRY: UK, COMPANY: Capitol, PRICE: 8.90, YEAR: 1983\n",
      "TITLE: Pavarotti Gala Concert, ARTIST: Luciano Pavarotti, COUNTRY: UK, COMPANY: DECCA, PRICE: 9.90, YEAR: 1991\n",
      "TITLE: Red, ARTIST: The Communards, COUNTRY: UK, COMPANY: London, PRICE: 7.80, YEAR: 1987\n"
     ]
    }
   ],
   "source": [
    "british_cds = cds.findall(\"CD[COUNTRY='UK']\")\n",
    "for bcd in british_cds:\n",
    "  display_cd(bcd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a05f0b",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "This code uses XPath to find all British CDs. It does this by selecting all `CD` tags which have a sub-tag `COUNTRY` with the text value `UK`.\n",
    "\n",
    "Reference: [XPath section of the ElementTree docs](https://docs.python.org/3/library/xml.etree.elementtree.html#xpath-support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63660b",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda5235",
   "metadata": {},
   "source": [
    "# Exercice 3 - Analyze JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b24d30",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Write a Python program that gets the file of filming locations in Paris at: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de719209",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.786738Z",
     "start_time": "2024-10-11T07:03:31.778307Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://opendata.paris.fr/explore/dataset/lieux-de-tournage-a-paris/download/?format=json&timezone=Europe/Berlin&lang=fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343174e2",
   "metadata": {},
   "source": [
    "- How many entries have you got?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3e1f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuboshu/opt/anaconda3/envs/bima/lib/python3.12/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'opendata.paris.fr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry count: 12265\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def download(source_url, dest_file):\n",
    "  s = Session()\n",
    "  s.verify = False\n",
    "  r = s.get(source_url, stream=True)\n",
    "  dest_file = Path(dest_file)\n",
    "\n",
    "  with open(dest_file, 'wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size=8192):\n",
    "      if chunk:\n",
    "        f.write(chunk)\n",
    "\n",
    "FN = 'tournage.json'\n",
    "download(url, FN)\n",
    "\n",
    "with open(FN) as f:\n",
    "  locs = json.load(f)\n",
    "\n",
    "print('Entry count:', len(locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378352d",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "This code makes use of the sample `download` function from the slides. The JSON file is downloaded to `tournage.json`, which is then re-opened to analyze. Since there is an array at the root, `len` is simply called on the loaded JSON to get the entry count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1eca6",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d23f6",
   "metadata": {},
   "source": [
    "- Analyze the JSON file: what is its structure?\n",
    "- Write a function that converts an entry in a string that shows director, title, district, start date, end date, and geographic coordinates.\n",
    "- Convert all entries in strings (warning: some entries may have issues).\n",
    "- Display the first 20 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f051f79",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f48e3fd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.576923Z",
     "start_time": "2024-10-11T07:03:35.572252Z"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_loc(entry):\n",
    "    fields = entry['fields']\n",
    "    director = fields.get('nom_realisateur', '<director missing>')\n",
    "    title = fields.get('nom_tournage', '<title missing>')\n",
    "    district = fields.get('ardt_lieu', '<district missing>')\n",
    "    start_date = fields.get('date_debut', '<start date missing>')\n",
    "    end_date = fields.get('date_fin', '<end date missing>')\n",
    "    coord_x = fields.get('coord_x', '<x coordinate missing>')\n",
    "    coord_y = fields.get('coord_y', '<y coordinate missing>')\n",
    "\n",
    "    return f\"{director}'s \\\"{title},\\\" filmed in {district} ({coord_x}, {coord_y}) from {start_date} to {end_date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb5f54",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "Metadata for each entry is stored in the `'fields'` key, however there may be missing fields for each entry. To safeguard for this, `dict.get` is used to give a default value in the case of a missing key.\n",
    "\n",
    "### File structure\n",
    "\n",
    "The JSON structure is an array of entries. The following is a formatted entry, to give an example of real data:\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"datasetid\":\"lieux-de-tournage-a-paris\",\n",
    "   \"recordid\":\"0ff321c5b140a12a8e50a1b212a7c5f5bced91d7\",\n",
    "   \"fields\":{\n",
    "      \"coord_x\":2.37006242,\n",
    "      \"id_lieu\":\"2017-751\",\n",
    "      \"adresse_lieu\":\"rue du faubourg du temple, 75011 paris\",\n",
    "      \"geo_shape\":{\n",
    "         \"coordinates\":[\n",
    "            2.370062415669748,\n",
    "            48.8696979988026\n",
    "         ],\n",
    "         \"type\":\"Point\"\n",
    "      },\n",
    "      \"coord_y\":48.869698,\n",
    "      \"ardt_lieu\":\"75011\",\n",
    "      \"nom_tournage\":\"2 Fils (Nouvelle Demande DÃ©cor Librairie / JournÃ©es interverties)\",\n",
    "      \"nom_realisateur\":\"FÃ©lix MOATI\",\n",
    "      \"date_debut\":\"2017-10-19\",\n",
    "      \"type_tournage\":\"Long mÃ©trage\",\n",
    "      \"annee_tournage\":\"2017\",\n",
    "      \"nom_producteur\":\"NORD OUEST FILMS\",\n",
    "      \"date_fin\":\"2017-10-19\",\n",
    "      \"geo_point_2d\":[\n",
    "         48.8696979988026,\n",
    "         2.370062415669748\n",
    "      ]\n",
    "   },\n",
    "   \"geometry\":{\n",
    "      \"type\":\"Point\",\n",
    "      \"coordinates\":[\n",
    "         2.370062415669748,\n",
    "         48.8696979988026\n",
    "      ]\n",
    "   },\n",
    "   \"record_timestamp\":\"2024-01-31T13:40:46.402+01:00\"\n",
    "}\n",
    "```\n",
    "\n",
    "Each entry may be missing specific keys from `\"fields\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f86ee18d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.670903Z",
     "start_time": "2024-10-11T07:03:35.614408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNE FONTAINE's \"POLICE,\" filmed in 75012 (2.39934074, 48.83798025) from 2019-03-08 to 2019-03-09\n",
      "Eli Ben-David's \"L'AttachÃ©,\" filmed in 75018 (2.34443461, 48.88730126) from 2019-03-14 to 2019-03-14\n",
      "Marc RECUENCO's \"En attendant qui ? Mai,\" filmed in 75017 (2.30595278, 48.8835646) from 2019-06-11 to 2019-06-11\n",
      "JEAN PASCAL ZADI ET JOHN WAXXX's \"TOUT SIMPLEMENT NOIR,\" filmed in 75005 (2.35024547, 48.84859142) from 2019-05-23 to 2019-05-23\n",
      "Nicolas Herdt's \"Une famille formidable,\" filmed in 75003 (2.36365029, 48.8602504) from 2018-08-06 to 2018-08-06\n",
      "Nicolas Herdt's \"Une famille formidable,\" filmed in 75003 (2.3621555, 48.86295435) from 2018-08-06 to 2018-08-06\n",
      "MaÃ¯mouna DoucourÃ©'s \"Les Mignonnes,\" filmed in 75019 (2.38208807, 48.88213499) from 2018-08-07 to 2018-08-07\n",
      "CHRISTOPHE BARRAUD's \"LEBOWITZ CONTRE LEBOWITZ/9 A 12,\" filmed in 75013 (2.359355, 48.838779) from 2016-11-09 to 2016-11-09\n",
      "NICOLAS HERDT's \"LEO MATTEI/14 ET 15,\" filmed in 75004 (2.365669, 48.84726) from 2016-10-06 to 2016-10-06\n",
      "JEANNE HERRY's \"10%/SAISON 2,\" filmed in 75019 (2.384688, 48.873777) from 2016-09-14 to 2016-09-14\n",
      "DAVID MICHOD's \"WAR MACHINE,\" filmed in 75001 (2.338081, 48.861863) from 2016-01-30 to 2016-01-30\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75013 (2.35076218, 48.8288281) from 2020-12-22 to 2020-12-22\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75013 (2.35137819, 48.82793218) from 2020-12-22 to 2020-12-22\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75013 (2.35137819, 48.82793218) from 2020-12-22 to 2020-12-22\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75001 (2.34748746, 48.86046195) from 2020-12-22 to 2020-12-22\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75001 (2.34772227, 48.86094047) from 2020-12-22 to 2020-12-22\n",
      "Roman Polanski's \"J'ACCUSE,\" filmed in 75015 (2.30996607, 48.84698001) from 2019-02-25 to 2019-02-25\n",
      "Alexandre Laurent's \"Le Bazar de la CharitÃ©,\" filmed in 75008 (2.31245551, 48.87963777) from 2019-03-14 to 2019-03-15\n",
      "Alexandre Laurent's \"Le Bazar de la CharitÃ©,\" filmed in 75008 (2.31123909, 48.88051652) from 2019-03-18 to 2019-03-18\n",
      "Maxime Roy's \"LES HEROIQUES,\" filmed in 75010 (2.36143507, 48.88235204) from 2019-03-11 to 2019-03-12\n"
     ]
    }
   ],
   "source": [
    "all_entries = [display_loc(e) for e in locs]\n",
    "print('\\n'.join(all_entries[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bb085",
   "metadata": {},
   "source": [
    "- A same movie can have multiple shooting locations. Make a list of movies, where each entry contains the movie title, its director, and shootings locations (district, start date, end date).\n",
    "- How many movies do you have?\n",
    "- Write a function that converts a movie into a string that shows director, title, and shootings.\n",
    "- Convert all movies in strings.\n",
    "- Display the first 20 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023a234",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f65854ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.675898Z",
     "start_time": "2024-10-11T07:03:35.672816Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, TypeVar, List\n",
    "\n",
    "Movie = TypeVar('Movie')\n",
    "movies: Dict[str, Movie] = dict()\n",
    "\n",
    "for loc in locs:\n",
    "  title = loc['fields']['nom_tournage']\n",
    "  if title not in movies:\n",
    "    movies[title] = {\n",
    "      'title': title,\n",
    "      'director': loc['fields'].get('nom_realisateur', '<director missing>'),\n",
    "      'shootings': []\n",
    "    }\n",
    "  movies[title]['shootings'].append({\n",
    "    'district': loc['fields'].get('ardt_lieu', '<arrondissement missing>'),\n",
    "    'start_date': loc['fields']['date_debut'],\n",
    "    'end_date': loc['fields']['date_fin']\n",
    "  })\n",
    "\n",
    "# Regroup locations per movie\n",
    "movies: List[Movie] = [m for m in movies.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d017c",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The question asks for two tasks to be accomplished:\n",
    "1. Entries are grouped by which movie they are a part of\n",
    "2. A subset of fields is displayed from each movie, including the newly aggregated field of shooting locations\n",
    "\n",
    "The most straightforward way to create this aggregation is via a dictionary. The movie title is chosen as the key, as there are no better unique identifier fields referencing the movie itself. \n",
    "\n",
    "While this organization is being done, the opportunity is taken to normalize the data into a new structure containing exactly what we need, and with no fields missing:\n",
    "\n",
    "```json\n",
    "A Movie is a dictionary with the schema:\n",
    "\n",
    "{\n",
    "  \"title\": \"string\",\n",
    "  \"director\": \"string\",\n",
    "  \"shootings\": [\n",
    "    {\n",
    "      \"district\": \"string\",\n",
    "      \"start_date\": \"string\",\n",
    "      \"end_date\": \"string\"\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Since the top-level dictionary was only needed for the process of organization, and not for the final data representation, we re-organize all of its values into a list for the final `movies` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cb17c8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.728960Z",
     "start_time": "2024-10-11T07:03:35.721811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1476"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c07db010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.740289Z",
     "start_time": "2024-10-11T07:03:35.730968Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def display_movie(movie):\n",
    "    movie_str = f\"{movie['director']}'s \\\"{movie['title']},\\\" was filmed in the following locations:\\n\"\n",
    "    for shooting in movie['shootings']:\n",
    "        movie_str += f'- {shooting['district']} between {shooting['start_date']} and {shooting['end_date']}\\n'\n",
    "    return movie_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5e8374d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.773580Z",
     "start_time": "2024-10-11T07:03:35.753086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNE FONTAINE's \"POLICE,\" was filmed in the following locations:\n",
      "- 75012 between 2019-03-08 and 2019-03-09\n",
      "- 75012 between 2019-03-08 and 2019-03-09\n",
      "- 75012 between 2019-04-10 and 2019-04-11\n",
      "- 75012 between 2019-03-11 and 2019-03-12\n",
      "- 75020 between 2019-03-27 and 2019-03-27\n",
      "- 75012 between 2019-03-07 and 2019-03-08\n",
      "- 75011 between 2019-03-27 and 2019-03-27\n",
      "- 75019 between 2019-03-28 and 2019-03-28\n",
      "- 75012 between 2019-03-25 and 2019-03-25\n",
      "- 75012 between 2019-03-28 and 2019-03-28\n",
      "- 75019 between 2019-04-08 and 2019-04-09\n",
      "\n",
      "Eli Ben-David's \"L'AttachÃ©,\" was filmed in the following locations:\n",
      "- 75018 between 2019-03-14 and 2019-03-14\n",
      "- 75018 between 2019-03-14 and 2019-03-14\n",
      "- 75005 between 2019-03-15 and 2019-03-15\n",
      "- 75012 between 2019-03-12 and 2019-03-12\n",
      "- 75009 between 2019-03-12 and 2019-03-12\n",
      "- 75001 between 2019-03-12 and 2019-03-12\n",
      "- 75004 between 2019-03-20 and 2019-03-20\n",
      "- 75001 between 2019-03-15 and 2019-03-16\n",
      "- 75004 between 2019-03-16 and 2019-03-16\n",
      "- 75005 between 2019-03-12 and 2019-03-12\n",
      "- 75010 between 2019-03-12 and 2019-03-12\n",
      "- 75007 between 2019-03-13 and 2019-03-13\n",
      "- 75010 between 2019-03-12 and 2019-03-12\n",
      "- 75004 between 2019-03-16 and 2019-03-16\n",
      "- 75018 between 2019-03-14 and 2019-03-15\n",
      "- 75003 between 2019-03-16 and 2019-03-16\n",
      "- 75018 between 2019-03-14 and 2019-03-14\n",
      "- 75001 between 2019-03-21 and 2019-03-22\n",
      "- 75004 between 2019-03-22 and 2019-03-23\n",
      "\n",
      "Marc RECUENCO's \"En attendant qui ? Mai,\" was filmed in the following locations:\n",
      "- 75017 between 2019-06-11 and 2019-06-11\n",
      "- 75017 between 2019-06-10 and 2019-06-10\n",
      "- 75017 between 2019-06-15 and 2019-06-15\n",
      "- 75017 between 2019-06-22 and 2019-06-23\n",
      "- 75005 between 2019-06-17 and 2019-06-18\n",
      "- 75005 between 2019-06-23 and 2019-06-23\n",
      "- 75017 between 2019-06-13 and 2019-06-13\n",
      "- 75017 between 2019-06-06 and 2019-06-07\n",
      "- 75013 between 2019-06-22 and 2019-06-22\n",
      "- 75017 between 2019-06-15 and 2019-06-15\n",
      "- 75017 between 2019-06-22 and 2019-06-23\n",
      "- 75017 between 2019-06-11 and 2019-06-11\n",
      "- 75017 between 2019-06-07 and 2019-06-07\n",
      "- 75017 between 2019-06-11 and 2019-06-11\n",
      "- 75017 between 2019-06-22 and 2019-06-23\n",
      "- 75005 between 2019-06-17 and 2019-06-17\n",
      "\n",
      "JEAN PASCAL ZADI ET JOHN WAXXX's \"TOUT SIMPLEMENT NOIR,\" was filmed in the following locations:\n",
      "- 75005 between 2019-05-23 and 2019-05-23\n",
      "- 75008 between 2019-06-12 and 2019-06-12\n",
      "- 75010 between 2019-05-30 and 2019-05-30\n",
      "- 75019 between 2019-06-26 and 2019-06-26\n",
      "- 75008 between 2019-06-13 and 2019-06-13\n",
      "- 75020 between 2019-06-04 and 2019-06-04\n",
      "- 75001 between 2019-06-28 and 2019-06-28\n",
      "- 75015 between 2019-07-05 and 2019-07-05\n",
      "- 75007 between 2019-07-05 and 2019-07-05\n",
      "- 75002 between 2019-06-16 and 2019-06-17\n",
      "- 75020 between 2019-06-04 and 2019-06-04\n",
      "- 75007 between 2019-05-30 and 2019-05-31\n",
      "- 75002 between 2019-06-26 and 2019-06-26\n",
      "- 75009 between 2019-06-24 and 2019-06-24\n",
      "- 75011 between 2019-07-10 and 2019-07-10\n",
      "- 75007 between 2019-07-05 and 2019-07-05\n",
      "- 75007 between 2019-05-31 and 2019-05-31\n",
      "- 75001 between 2019-06-26 and 2019-06-26\n",
      "- 75015 between 2019-06-03 and 2019-06-03\n",
      "- 75015 between 2019-07-05 and 2019-07-05\n",
      "- 75002 between 2019-06-12 and 2019-06-12\n",
      "- 75017 between 2019-06-25 and 2019-06-25\n",
      "- 75006 between 2019-07-10 and 2019-07-11\n",
      "- 75007 between 2019-07-05 and 2019-07-05\n",
      "- 75005 between 2019-05-23 and 2019-05-23\n",
      "- 75009 between 2019-05-22 and 2019-05-23\n",
      "- 75008 between 2019-06-28 and 2019-06-28\n",
      "- 75004 between 2019-05-23 and 2019-05-23\n",
      "- 75002 between 2019-06-24 and 2019-06-24\n",
      "- 75116 between 2019-06-28 and 2019-06-28\n",
      "- 75010 between 2019-05-29 and 2019-05-30\n",
      "- 75002 between 2019-06-12 and 2019-06-12\n",
      "- 75002 between 2019-06-24 and 2019-06-24\n",
      "- 75001 between 2019-06-24 and 2019-06-24\n",
      "- 75007 between 2019-05-31 and 2019-05-31\n",
      "- 75004 between 2019-05-20 and 2019-05-20\n",
      "- 75009 between 2019-06-12 and 2019-06-12\n",
      "\n",
      "Nicolas Herdt's \"Une famille formidable,\" was filmed in the following locations:\n",
      "- 75003 between 2018-08-06 and 2018-08-06\n",
      "- 75003 between 2018-08-06 and 2018-08-06\n",
      "- 75007 between 2018-08-13 and 2018-08-13\n",
      "- 75013 between 2018-08-24 and 2018-08-24\n",
      "- 75006 between 2018-08-31 and 2018-08-31\n",
      "- 75006 between 2018-08-31 and 2018-08-31\n",
      "- 75003 between 2018-08-07 and 2018-08-08\n",
      "- 75007 between 2018-08-13 and 2018-08-13\n",
      "\n",
      "MaÃ¯mouna DoucourÃ©'s \"Les Mignonnes,\" was filmed in the following locations:\n",
      "- 75019 between 2018-08-07 and 2018-08-07\n",
      "- 75019 between 2018-08-22 and 2018-08-27\n",
      "- 75019 between 2018-08-06 and 2018-08-06\n",
      "- 75019 between 2018-08-06 and 2018-08-06\n",
      "- 75019 between 2018-08-06 and 2018-08-06\n",
      "- 75019 between 2018-08-08 and 2018-08-08\n",
      "- 75019 between 2018-08-08 and 2018-08-08\n",
      "- 75019 between 2018-08-22 and 2018-08-27\n",
      "\n",
      "CHRISTOPHE BARRAUD's \"LEBOWITZ CONTRE LEBOWITZ/9 A 12,\" was filmed in the following locations:\n",
      "- 75013 between 2016-11-09 and 2016-11-09\n",
      "- 75018 between 2016-11-04 and 2016-11-04\n",
      "- 75011 between 2016-10-31 and 2016-10-31\n",
      "- 75002 between 2016-11-01 and 2016-11-01\n",
      "- 75018 between 2016-11-04 and 2016-11-04\n",
      "- 75011 between 2016-11-02 and 2016-11-02\n",
      "- 75013 between 2016-11-09 and 2016-11-09\n",
      "- 75011 between 2016-10-31 and 2016-10-31\n",
      "- 75010 between 2016-10-27 and 2016-10-27\n",
      "- 75010 between 2016-10-27 and 2016-10-27\n",
      "- 75010 between 2016-10-27 and 2016-10-27\n",
      "- 75004 between 2016-10-24 and 2016-10-24\n",
      "- 75016 between 2016-10-25 and 2016-10-26\n",
      "- 75018 between 2016-11-04 and 2016-11-04\n",
      "- 75018 between 2016-11-04 and 2016-11-04\n",
      "\n",
      "NICOLAS HERDT's \"LEO MATTEI/14 ET 15,\" was filmed in the following locations:\n",
      "- 75004 between 2016-10-06 and 2016-10-06\n",
      "- 75003 between 2016-10-05 and 2016-10-05\n",
      "- 75004 between 2016-10-06 and 2016-10-06\n",
      "- 75003 between 2016-10-05 and 2016-10-05\n",
      "- 75010 between 2016-10-18 and 2016-10-18\n",
      "- 75010 between 2016-10-18 and 2016-10-18\n",
      "- 75003 between 2016-10-05 and 2016-10-05\n",
      "- 75012 between 2016-10-07 and 2016-10-07\n",
      "- 75009 between 2016-10-01 and 2016-10-01\n",
      "- 75004 between 2016-10-06 and 2016-10-06\n",
      "\n",
      "JEANNE HERRY's \"10%/SAISON 2,\" was filmed in the following locations:\n",
      "- 75019 between 2016-09-14 and 2016-09-14\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75008 between 2016-09-12 and 2016-09-12\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75008 between 2016-09-13 and 2016-09-13\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75008 between 2016-10-17 and 2016-10-18\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75008 between 2016-10-14 and 2016-10-14\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75002 between 2016-10-03 and 2016-10-03\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75020 between 2016-09-14 and 2016-09-14\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75016 between 2016-10-10 and 2016-10-10\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75019 between 2016-09-16 and 2016-09-16\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75008 between 2016-10-11 and 2016-10-11\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75002 between 2016-09-19 and 2016-09-19\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75016 between 2016-10-05 and 2016-10-05\n",
      "- 75008 between 2016-09-13 and 2016-09-13\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75016 between 2016-10-06 and 2016-10-06\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75019 between 2016-09-16 and 2016-09-16\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "\n",
      "DAVID MICHOD's \"WAR MACHINE,\" was filmed in the following locations:\n",
      "- 75001 between 2016-01-30 and 2016-01-30\n",
      "- 75009 between 2016-01-30 and 2016-01-30\n",
      "- 75001 between 2016-01-31 and 2016-01-31\n",
      "- 75009 between 2016-01-31 and 2016-01-31\n",
      "- 75001 between 2016-01-31 and 2016-01-31\n",
      "- 75009 between 2016-01-31 and 2016-01-31\n",
      "- 75008 between 2016-01-31 and 2016-01-31\n",
      "- 75001 between 2016-01-31 and 2016-01-31\n",
      "- 75007 between 2016-01-30 and 2016-01-30\n",
      "\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" was filmed in the following locations:\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2021-02-24 and 2021-02-24\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75003 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-18 and 2021-02-18\n",
      "- 75013 between 2021-02-22 and 2021-02-22\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75003 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2021-02-11 and 2021-02-12\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75001 between 2021-02-15 and 2021-02-15\n",
      "- 75005 between 2021-02-24 and 2021-02-24\n",
      "- 75013 between 2021-02-25 and 2021-02-26\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2021-02-11 and 2021-02-12\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75003 between 2021-02-09 and 2021-02-10\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75012 between 2021-02-25 and 2021-02-25\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-18 and 2021-02-18\n",
      "- 75005 between 2021-02-24 and 2021-02-24\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75001 between 2021-02-26 and 2021-02-26\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2021-02-10 and 2021-02-11\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75013 between 2021-02-22 and 2021-02-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75005 between 2021-02-24 and 2021-02-24\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2021-02-12 and 2021-02-13\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75001 between 2021-02-15 and 2021-02-15\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75001 between 2021-02-26 and 2021-02-26\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "\n",
      "Roman Polanski's \"J'ACCUSE,\" was filmed in the following locations:\n",
      "- 75015 between 2019-02-25 and 2019-02-25\n",
      "- 75009 between 2019-02-13 and 2019-02-15\n",
      "- 75009 between 2019-01-31 and 2019-02-01\n",
      "- 75009 between 2019-02-13 and 2019-02-15\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75015 between 2019-02-26 and 2019-02-27\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75015 between 2019-02-22 and 2019-02-22\n",
      "- 75001 between 2019-01-29 and 2019-01-29\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75016 between 2019-02-20 and 2019-02-21\n",
      "- 75006 between 2019-03-04 and 2019-03-04\n",
      "- 75015 between 2019-02-21 and 2019-02-22\n",
      "\n",
      "Alexandre Laurent's \"Le Bazar de la CharitÃ©,\" was filmed in the following locations:\n",
      "- 75008 between 2019-03-14 and 2019-03-15\n",
      "- 75008 between 2019-03-18 and 2019-03-18\n",
      "- 75016 between 2019-02-18 and 2019-02-24\n",
      "- 75004 between 2019-03-07 and 2019-03-07\n",
      "- 75016 between 2019-02-23 and 2019-02-27\n",
      "- 75004 between 2019-03-04 and 2019-03-05\n",
      "- 75004 between 2019-03-08 and 2019-03-08\n",
      "- 75008 between 2019-03-19 and 2019-03-19\n",
      "- 75008 between 2019-03-14 and 2019-03-16\n",
      "- 75016 between 2019-02-11 and 2019-02-19\n",
      "- 75008 between 2019-03-15 and 2019-03-16\n",
      "- 75008 between 2019-03-11 and 2019-03-16\n",
      "- 75004 between 2019-03-06 and 2019-03-06\n",
      "- 75008 between 2019-03-11 and 2019-03-12\n",
      "- 75005 between 2019-03-05 and 2019-03-06\n",
      "- 75004 between 2019-03-07 and 2019-03-07\n",
      "- 75005 between 2019-03-04 and 2019-03-04\n",
      "- 75008 between 2019-03-13 and 2019-03-13\n",
      "- 75005 between 2019-03-05 and 2019-03-05\n",
      "\n",
      "Maxime Roy's \"LES HEROIQUES,\" was filmed in the following locations:\n",
      "- 75010 between 2019-03-11 and 2019-03-12\n",
      "\n",
      "PHILIPPE LIORET's \"PARIS BREST,\" was filmed in the following locations:\n",
      "- 75006 between 2019-03-15 and 2019-03-15\n",
      "- 75006 between 2019-03-15 and 2019-03-15\n",
      "\n",
      "Damien Chazelle's \"The Eddy,\" was filmed in the following locations:\n",
      "- 75013 between 2019-05-28 and 2019-05-28\n",
      "- 75012 between 2019-05-28 and 2019-05-28\n",
      "- 75013 between 2019-05-15 and 2019-05-16\n",
      "- 75012 between 2019-05-28 and 2019-05-28\n",
      "- 75020 between 2019-10-11 and 2019-10-12\n",
      "- 75016 between 2019-05-24 and 2019-05-24\n",
      "- 75016 between 2019-09-02 and 2019-09-02\n",
      "- 75012 between 2019-06-17 and 2019-06-20\n",
      "- 75012 between 2019-08-21 and 2019-08-31\n",
      "- 75020 between 2019-08-20 and 2019-08-20\n",
      "- 75012 between 2019-10-03 and 2019-10-04\n",
      "- 75012 between 2019-09-13 and 2019-09-13\n",
      "- 75016 between 2019-05-24 and 2019-05-24\n",
      "- 75020 between 2019-05-16 and 2019-05-17\n",
      "- 75020 between 2019-09-30 and 2019-10-02\n",
      "- 75020 between 2019-08-14 and 2019-08-14\n",
      "- 75004 between 2019-08-19 and 2019-08-19\n",
      "- 75020 between 2019-04-11 and 2019-04-11\n",
      "- 75013 between 2019-05-28 and 2019-05-29\n",
      "- 75012 between 2019-05-31 and 2019-06-01\n",
      "- 75011 between 2019-08-20 and 2019-08-21\n",
      "- 75014 between 2019-05-22 and 2019-05-22\n",
      "- 75013 between 2019-05-28 and 2019-05-28\n",
      "- 75016 between 2019-08-09 and 2019-08-09\n",
      "- 75012 between 2019-06-11 and 2019-06-11\n",
      "- 75020 between 2019-05-07 and 2019-05-07\n",
      "- 75012 between 2019-09-18 and 2019-09-20\n",
      "- 75020 between 2019-09-30 and 2019-10-01\n",
      "- 75013 between 2019-05-28 and 2019-05-28\n",
      "- 75019 between 2019-05-07 and 2019-05-08\n",
      "- 75010 between 2019-08-20 and 2019-08-22\n",
      "- 75019 between 2019-06-24 and 2019-06-25\n",
      "- 75013 between 2019-05-27 and 2019-05-29\n",
      "- 75012 between 2019-05-28 and 2019-05-28\n",
      "- 75012 between 2019-06-04 and 2019-06-04\n",
      "- 75014 between 2019-08-06 and 2019-08-08\n",
      "- 75012 between 2019-07-15 and 2019-07-16\n",
      "- 75014 between 2019-05-23 and 2019-05-24\n",
      "- 75018 between 2019-06-28 and 2019-06-28\n",
      "- 75012 between 2019-06-05 and 2019-06-05\n",
      "- 75116 between 2019-09-02 and 2019-09-03\n",
      "- 75014 between 2019-07-16 and 2019-07-17\n",
      "- 75019 between 2019-08-20 and 2019-08-20\n",
      "- 75012 between 2019-09-17 and 2019-09-17\n",
      "- 75014 between 2019-05-23 and 2019-05-23\n",
      "- 75012 between 2019-09-12 and 2019-09-12\n",
      "- 75012 between 2019-09-17 and 2019-09-17\n",
      "- 75116 between 2019-09-24 and 2019-09-24\n",
      "- 75013 between 2019-05-27 and 2019-05-28\n",
      "- 75012 between 2019-05-30 and 2019-05-30\n",
      "- 75007 between 2019-04-30 and 2019-05-01\n",
      "- 75020 between 2019-05-07 and 2019-05-08\n",
      "- 75012 between 2019-05-29 and 2019-05-29\n",
      "- 75016 between 2019-09-24 and 2019-09-24\n",
      "- 75014 between 2019-05-22 and 2019-05-24\n",
      "- 75019 between 2019-04-11 and 2019-04-11\n",
      "- 75010 between 2019-07-04 and 2019-07-04\n",
      "- 75012 between 2019-06-26 and 2019-06-26\n",
      "- 75002 between 2019-07-01 and 2019-07-02\n",
      "\n",
      "MARTIN PROVOST's \"LA SAGE FEMME,\" was filmed in the following locations:\n",
      "- 75008 between 2016-04-11 and 2016-04-11\n",
      "- 75008 between 2016-04-05 and 2016-04-06\n",
      "- 75018 between 2016-03-31 and 2016-03-31\n",
      "- 75005 between 2016-04-13 and 2016-04-13\n",
      "- 75018 between 2016-05-24 and 2016-05-24\n",
      "- 75008 between 2016-04-11 and 2016-04-11\n",
      "- 75018 between 2016-03-31 and 2016-03-31\n",
      "- 75018 between 2016-05-24 and 2016-05-24\n",
      "- 75016 between 2016-05-20 and 2016-05-20\n",
      "- 75018 between 2016-05-24 and 2016-05-24\n",
      "- 75005 between 2016-04-12 and 2016-04-12\n",
      "- 75008 between 2016-04-11 and 2016-04-11\n",
      "- 75018 between 2016-05-24 and 2016-05-24\n",
      "- 75008 between 2016-04-11 and 2016-04-11\n",
      "- 75005 between 2016-04-13 and 2016-04-13\n",
      "- 75016 between 2016-05-04 and 2016-05-04\n",
      "- 75008 between 2016-04-07 and 2016-04-07\n",
      "\n",
      "Jim Bagdonas's \"MODERN FAMILY 11,\" was filmed in the following locations:\n",
      "- 75005 between 2019-11-15 and 2019-11-15\n",
      "- 75004 between 2019-11-15 and 2019-11-15\n",
      "- 75004 between 2019-11-15 and 2019-11-15\n",
      "- 75005 between 2019-11-15 and 2019-11-15\n",
      "- 75004 between 2019-11-15 and 2019-11-15\n",
      "\n",
      "julien ZIDI's \"Alice NEVERS,\" was filmed in the following locations:\n",
      "- 75116 between 2019-11-19 and 2019-11-19\n",
      "- 75116 between 2019-11-21 and 2019-11-21\n",
      "- 75001 between 2019-11-16 and 2019-11-16\n",
      "- 75009 between 2018-11-20 and 2018-11-21\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75116 between 2019-11-22 and 2019-11-23\n",
      "- 75007 between 2019-11-25 and 2019-11-25\n",
      "- 75006 between 2018-11-19 and 2018-11-20\n",
      "- 75019 between 2019-11-29 and 2019-11-29\n",
      "- 75019 between 2019-11-28 and 2019-11-28\n",
      "- 75116 between 2019-11-18 and 2019-11-21\n",
      "- 75116 between 2018-11-16 and 2018-11-16\n",
      "- 75019 between 2019-11-29 and 2019-11-29\n",
      "- 75001 between 2019-11-13 and 2019-11-13\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75001 between 2019-11-16 and 2019-11-17\n",
      "- 75001 between 2019-11-16 and 2019-11-16\n",
      "- 75019 between 2019-11-28 and 2019-11-28\n",
      "\n",
      "Katia LEWKOWICZ's \"FORTE,\" was filmed in the following locations:\n",
      "- 75018 between 2018-12-07 and 2018-12-08\n",
      "- 75019 between 2018-12-05 and 2018-12-06\n",
      "- 75018 between 2018-11-08 and 2018-11-08\n",
      "- 75008 between 2018-11-13 and 2018-11-13\n",
      "- 75009 between 2018-12-20 and 2018-12-21\n",
      "- 75019 between 2018-11-08 and 2018-11-08\n",
      "- 75008 between 2018-11-09 and 2018-11-16\n",
      "- 75018 between 2018-12-07 and 2018-12-07\n",
      "- 75001 between 2018-11-28 and 2018-11-28\n",
      "- 75020 between 2018-11-29 and 2018-11-29\n",
      "- 75008 between 2018-11-16 and 2018-11-16\n",
      "- 75009 between 2018-11-27 and 2018-11-27\n",
      "- 75001 between 2018-11-28 and 2018-11-28\n",
      "- 75020 between 2018-12-20 and 2018-12-20\n",
      "- 75019 between 2018-11-20 and 2018-11-26\n",
      "- 75018 between 2018-12-10 and 2018-12-10\n",
      "- 75018 between 2018-12-11 and 2018-12-11\n",
      "- 75008 between 2018-11-14 and 2018-11-14\n",
      "- 75019 between 2018-11-22 and 2018-11-22\n",
      "- 75116 between 2018-12-06 and 2018-12-08\n",
      "- 75001 between 2018-11-28 and 2018-11-28\n",
      "- 75116 between 2018-12-06 and 2018-12-08\n",
      "- 75018 between 2018-12-13 and 2018-12-20\n",
      "- 75013 between 2018-12-03 and 2018-12-04\n",
      "- 75018 between 2018-12-10 and 2018-12-12\n",
      "- 75020 between 2018-11-30 and 2018-11-30\n",
      "- 75020 between 2018-11-30 and 2018-11-30\n",
      "- 75019 between 2018-12-05 and 2018-12-05\n",
      "- 75020 between 2018-11-29 and 2018-11-29\n",
      "- 75020 between 2018-11-29 and 2018-11-29\n",
      "- 75009 between 2018-12-20 and 2018-12-20\n",
      "- 75008 between 2018-11-09 and 2018-11-09\n",
      "- 75020 between 2018-12-20 and 2018-12-20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_movie_displays = [display_movie(m) for m in movies]\n",
    "print('\\n'.join(all_movie_displays[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188458f",
   "metadata": {},
   "source": [
    "- Display for each district its number of shootings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25c863",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbf057c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.806608Z",
     "start_time": "2024-10-11T07:03:35.793705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'75012': 596,\n",
       " '75020': 587,\n",
       " '75011': 641,\n",
       " '75019': 745,\n",
       " '75018': 1043,\n",
       " '75005': 640,\n",
       " '75009': 642,\n",
       " '75001': 722,\n",
       " '75004': 670,\n",
       " '75010': 749,\n",
       " '75007': 657,\n",
       " '75003': 236,\n",
       " '75017': 378,\n",
       " '75013': 658,\n",
       " '75008': 798,\n",
       " '75015': 363,\n",
       " '75002': 297,\n",
       " '75006': 471,\n",
       " '75116': 421,\n",
       " '75016': 614,\n",
       " '75014': 321,\n",
       " '94320': 4,\n",
       " '<arrondissement missing>': 1,\n",
       " '93500': 6,\n",
       " '93320': 1,\n",
       " '92220': 1,\n",
       " '92170': 1,\n",
       " '93200': 1,\n",
       " '93000': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def district_count_reducer(acc: Dict[str, int], movie: Movie) -> Dict[str, int]:\n",
    "  for shooting in movie['shootings']:\n",
    "    d = shooting['district']\n",
    "    if d not in acc:\n",
    "      acc[d] = 0\n",
    "    acc[d] += 1\n",
    "  return acc\n",
    "\n",
    "stats = functools.reduce(district_count_reducer, movies, dict())\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84df8db",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "This exercise asks to transform an array of `Movie`s to a hash mapping a piece of information within a `Movie` to an integer counting occurrences. This is a prime use-case for `reduce`, as we are changing the data type.\n",
    "\n",
    "We initialize a `reduce` call on `movies` with a function, and an initial value of an empty dictionary. The pieces of data we need to count for each `Movie` is in the `'shootings'` key, which is an array. Therefore we loop, and increment the accumulator key corresponding to the information we care about (`'district'`) for each shooting location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b58306",
   "metadata": {},
   "source": [
    "# Exercice 4 - Analyze CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2fe268",
   "metadata": {},
   "source": [
    "- Write a Python code retrieves the file of the most loaned titles in libraries in Paris at: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "663845ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.818501Z",
     "start_time": "2024-10-11T07:03:35.808870Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://opendata.paris.fr/explore/dataset/les-titres-les-plus-pretes/download/?format=csv&timezone=Europe/Berlin&lang=en&use_labels_for_header=true&csv_separator=%3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3296ee1",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9256ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get, Session\n",
    "from io import StringIO\n",
    "import csv\n",
    "s = Session()\n",
    "data = s.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ac1f0",
   "metadata": {},
   "source": [
    "### Explanlation\n",
    "\n",
    "This code retrieves CSV data from the provided URL using the `requests` library within a `session`, which handles persistent connections. After the data is fetched, it is stored as a csv string in the `data` variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d665083",
   "metadata": {},
   "source": [
    "- Analyze the resulting CSV file to display, for all entries: title, author, and total number of loans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c39b1",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22877be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.078350Z",
     "start_time": "2024-10-11T07:03:36.075015Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type de document;PrÃªts 2022;Titre;Auteur;Nombre de localisations;Nombre de prÃªt total;Nombre d'exemplaires\n",
      "Bande dessinÃ©e jeunesse;1064;Razzia;Sobral,  Patrick;47;2938;67\n",
      "Bande dessinÃ©e jeunesse;1024;Touche pas Ã  mon veau;Guibert,  Emmanuel;45;2296;71\n",
      "Bande dessinÃ©e jeunesse;1016;Max et Lili vont chez papy et mamie;Saint-Mars,  Dominique de;50;5554;103\n",
      "Bande dessinÃ©e jeunesse;938;Lili veut un petit chat;Saint-Mars,  Dominique de;51;5789;80\n",
      "Bande dessinÃ©e jeunesse;921;Max et Lili font du camping;Saint-Mars,  Dominique de;52;5658;83\n",
      "Bande dessinÃ©e jeunesse;901;Lili trouve sa maÃ®tresse mÃ©ch\n",
      "total len of data: 66061\n",
      "total entries: 842\n"
     ]
    }
   ],
   "source": [
    "print(data[:600])\n",
    "print(f'total len of data: {len(data)}')\n",
    "\n",
    "books = [] # Save all retrieved data\n",
    "with StringIO(data) as csvfile:\n",
    "    r = csv.reader(csvfile, delimiter=';')\n",
    "    for i, row in enumerate(r):\n",
    "        if i == 0: \n",
    "            # ignore first row: column names\n",
    "            continue\n",
    "        book = {\n",
    "        \"Type\": row[0],\n",
    "        \"Loans\": int(row[1]),\n",
    "        \"Title\": row[2],\n",
    "        \"Author\": row[3],\n",
    "        \"Area\": row[4],\n",
    "        \"Total_Loans\": int(row[5]),\n",
    "        \"Total_Copies\": int(row[6])\n",
    "        }\n",
    "        books.append(book)\n",
    "\n",
    "print(f'total entries: {len(books)}')\n",
    "\n",
    "def disp_book(book):\n",
    "    title = book['Title']\n",
    "    author = book['Author']\n",
    "    loans = book['Total_Loans']\n",
    "    return f'\"{title}\", by {author} ({loans} loans)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efad4fa",
   "metadata": {},
   "source": [
    "### Explanlation\n",
    "\n",
    "To inspect part of the data, the first 600 characters are printed along with the total length of the dataset. There are 7 columns shown in header: Type de document;PrÃªts 2022;Titre;Auteur;Nombre de localisations;Nombre de prÃªt total;Nombre d'exemplaires. \n",
    "\n",
    "Next, the CSV data is parsed using Python's csv.reader, and a list of dictionaries (`books`) is created to store the processed information. Each row (after the header) is treated as a separate book record. Since the delimiter is **';'** but not **','**, I searched the `csv.reader` method's documentation(https://docs.python.org/3/library/csv.html#csv-fmt-params) and found that `delimiter` parameter is used to specify customized delimiter.\n",
    "\n",
    "The first row, which contains the column headers, is skipped using `if i == 0`. Then, for each subsequent row, a dictionary is created with the following keys:\n",
    "\n",
    "- Type: Type de document\n",
    "- Loans: PrÃªts 2022 \n",
    "- Title: Titre\n",
    "- Author: Auteur\n",
    "- Area: Nombre de localisations\n",
    "- Total_Loans: Nombre de prÃªt total\n",
    "- Total_Copies: Nombre d'exemplaires\n",
    "  \n",
    "Each of these dictionaries is appended to the `books` list. Finally, the total number of books processed is printed using len(books).\n",
    "\n",
    "Since each entry corresponds to a dict element stored in the `books` list, displaying the title, author, and total number of loans is just looking up the relevant keys in each dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1b29eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.115744Z",
     "start_time": "2024-10-11T07:03:36.103623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Razzia\", by Sobral,  Patrick (2938 loans)\n",
      "\"Touche pas Ã  mon veau\", by Guibert,  Emmanuel (2296 loans)\n",
      "\"Max et Lili vont chez papy et mamie\", by Saint-Mars,  Dominique de (5554 loans)\n",
      "\"Lili veut un petit chat\", by Saint-Mars,  Dominique de (5789 loans)\n",
      "\"Max et Lili font du camping\", by Saint-Mars,  Dominique de (5658 loans)\n",
      "\"Lili trouve sa maÃ®tresse mÃ©chante\", by Saint-Mars,  Dominique de (4694 loans)\n",
      "\"J'irai oÃ¹ tu iras\", by Lyfoung,  Patricia (4707 loans)\n",
      "\"Les nerfs Ã  vif\", by Nob (2837 loans)\n",
      "\"Je crois que je t'aime\", by Lyfoung,  Patricia (3878 loans)\n",
      "\"Attention tornade\", by Cazenove,  Christophe (2366 loans)\n",
      "\"Max et Lili se posent des questions sur Dieu\", by Saint-Mars,  Dominique de (4823 loans)\n",
      "\"Game over. 13. Toxic affair\", by Midam (2652 loans)\n",
      "\"Les Schtroumpfs et la tempÃªte blanche\", by Jost,  Alain (975 loans)\n",
      "\"On a marchÃ© sur la lune\", by HergÃ© (5674 loans)\n",
      "\"AstÃ©rix chez les Bretons\", by Goscinny,  RenÃ© (3014 loans)\n",
      "\"Parvati\", by Ogaki,  Philippe (2616 loans)\n",
      "\"Les Schtroumpfs et l'arbre d'or\", by Culliford,  Thierry (3460 loans)\n",
      "\"La dÃ©cision : roman\", by Tuil,  Karine (976 loans)\n",
      "\"Les cahiers d'Esther. 4. Histoires de mes 13 ans\", by Sattouf,  Riad (2171 loans)\n",
      "\"Salut, les zinzins !\", by Cohen,  Jacqueline (4565 loans)\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join( [disp_book(b) for b in books[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1043a24",
   "metadata": {},
   "source": [
    "- Display for each type of document (there can be several entries for the same type of document), the total number of loans for this type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6077c6",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48b92fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.142096Z",
     "start_time": "2024-10-11T07:03:36.130485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bande dessinÃ©e jeunesse': 2300143,\n",
       " 'Livre adulte': 41731,\n",
       " 'Bande dessinÃ©e adulte': 59726,\n",
       " 'Livre sonore jeunesse': 10630,\n",
       " 'Livre jeunesse': 104067,\n",
       " 'Bande dessinÃ©e ado': 29819,\n",
       " 'DVD jeunesse': 2471,\n",
       " 'Jeux vidÃ©os tous publics Non prÃªtables': 4235,\n",
       " 'Jeux de sociÃ©tÃ© prÃªtable': 10057,\n",
       " 'Musique jeunesse': 4792,\n",
       " 'Jeux de sociÃ©tÃ©': 1753}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = {}\n",
    "for b in books:\n",
    "    if b['Type'] not in stats:\n",
    "        stats[b['Type']] = b['Total_Loans']\n",
    "    else:\n",
    "        stats[b['Type']] += b['Total_Loans']\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ebc88",
   "metadata": {},
   "source": [
    "### Explanlation\n",
    "\n",
    "This code calculates the total number of loans for each document type. It loops through the `books` list and checks if the document type (`b['Type']`) is already in the `stats` dictionary. If not, it adds the type and sets the total loans to the current bookâs `Total_Loans`. If the type is already in `stats`, it adds the current book's loans to the existing total. After the loop, `stats` contains the total number of loans for each document type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2853229",
   "metadata": {},
   "source": [
    "- Display titles in order of profitability (in descending order of the number of loans per copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26d95669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.167624Z",
     "start_time": "2024-10-11T07:03:36.158896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Console Nintendo Switch\" (1648 loans, 2 copies)\n",
      "\"Console PlayStation 4\" (2587 loans, 6 copies)\n",
      "\"SOS ouistiti :\" (1868 loans, 5 copies)\n",
      "\"Quatre en ligne :\" (1753 loans, 5 copies)\n",
      "\"Perplexus : : original\" (2254 loans, 8 copies)\n",
      "\"Un enfant chez les schtroumpfs\", by DÃ­az Vizoso,  Miguel (4504 loans, 43 copies)\n",
      "\"Mon meilleur ami\", by Verron,  Laurent (4662 loans, 47 copies)\n",
      "\"Les vacances infernales\", by Cohen,  Jacqueline (5014 loans, 51 copies)\n",
      "\"Bande de sauvages !\", by Cohen,  Jacqueline (5761 loans, 60 copies)\n",
      "\"Trop, c'est trop !\", by Cohen,  Jacqueline (4504 loans, 47 copies)\n",
      "\"Les fous du mercredi\", by Cohen,  Jacqueline (5169 loans, 54 copies)\n",
      "\"Ca va chauffer !\", by Cohen,  Jacqueline (4071 loans, 44 copies)\n",
      "\"Uno :\" (3136 loans, 34 copies)\n",
      "\"Ca roule !\", by Cohen,  Jacqueline (5763 loans, 63 copies)\n",
      "\"Salut, les zinzins !\", by Cohen,  Jacqueline (4565 loans, 50 copies)\n",
      "\"Les deux terreurs\", by Cohen,  Jacqueline (3999 loans, 44 copies)\n",
      "\"Subliiiimes !\", by Cohen,  Jacqueline (5007 loans, 56 copies)\n",
      "\"Un copieur sachant copier\", by Godi,  Bernard (3481 loans, 39 copies)\n",
      "\"A l'attaque !\", by Cohen,  Jacqueline (4353 loans, 49 copies)\n",
      "\"Tom-Tom et l'impossible Nana\", by Cohen,  Jacqueline (5832 loans, 66 copies)\n"
     ]
    }
   ],
   "source": [
    "def disp_book(book):\n",
    "    title = book['Title']\n",
    "    author = book['Author']\n",
    "    loans = book['Total_Loans']\n",
    "    copies = book['Total_Copies']\n",
    "    if author:\n",
    "        return f'\"{title}\", by {author} ({loans} loans, {copies} copies)'\n",
    "    else: \n",
    "        return f'\"{title}\" ({loans} loans, {copies} copies)'\n",
    "    \n",
    "\n",
    "for b in books:\n",
    "    b['Profitability'] = b['Total_Loans'] / b['Total_Copies']\n",
    "\n",
    "sorted_books = sorted(books, key=lambda x: x['Profitability'], reverse=True)\n",
    "print('\\n'.join( [disp_book(b) for b in sorted_books[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7016df",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "In this code, the `disp_book` function has been slightly modified to handle books that don't have an author. The function now displays the title, total loans, and total copies, and if the book has an author, it includes the author in the output. If the author is missing, it only displays the title, loans, and copies.\n",
    "\n",
    "The books are sorted by a new field called `Profitability`, which is calculated as the ratio of `Total_Loans` to `Total_Copies` for each book. The books with higher profitability are ranked higher.\n",
    "\n",
    "The `sorted` function, which was found through a search on Stack Overflow (https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value), is used to sort the `books` list in descending order of profitability. `key` parameter specifies a function to be called on each element before sorting. In this case, we use a lambda function: `lambda x: x['Profitability']`. This lambda function takes a book (x) and returns its Profitability value, which is used as the sorting criterion. Then `reverse=True` is used to sort the books in descending order, meaning the books with the highest profitability will appear first.\n",
    "\n",
    "Finally, the top 20 most profitable books are displayed using the `disp_book` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5668b",
   "metadata": {},
   "source": [
    "# Exercice 5 * - Analyze HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85237162",
   "metadata": {},
   "source": [
    "- Write a Python program that gets the content of the Wikipedia page at: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57dc6b89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.180211Z",
     "start_time": "2024-10-11T07:03:36.168632Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_density\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7e866",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271cbfd",
   "metadata": {},
   "source": [
    "- Display all the countries mentioned in the table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ff5df",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ad3f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.660044Z",
     "start_time": "2024-10-11T07:03:36.649884Z"
    }
   },
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32678327",
   "metadata": {},
   "source": [
    "- Display for each country its rank, density, population, area. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a9623",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3525d8",
   "metadata": {},
   "source": [
    "- Save the information obtained in a Python dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b00cbf",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c31344",
   "metadata": {},
   "source": [
    "- Using the previously saved Python dictionary, ask the user for a country, display the \n",
    "corresponding information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d58996",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de123f",
   "metadata": {},
   "source": [
    "# Exercice 6 * - API Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62855a4",
   "metadata": {},
   "source": [
    "- Write a Python program that will make available a Web API allowing elementary calculations on \n",
    "integers.\n",
    "\n",
    "The APIs are accessible by GET and in the form: \n",
    "- /add/{integer1}/{integer2}: add integer1 and integer2\n",
    "- /sub/{integer1}/{integer2}: perform the subtraction of integer1 and integer2\n",
    "- /mul/{integer1}/{integer2}: carry out the multiplication of integer1 and integer2\n",
    "- /div/{integer1}/{integer2}: perform the integer division of integer1 by integer2\n",
    "- /mod/{integer1}/{integer2}: perform the remainder of the integer division of integer1\n",
    "by integer2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad95976",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6b6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:46.583700Z",
     "start_time": "2024-10-11T07:03:36.825290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:8080\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [11/Oct/2024 09:03:41] \"GET /mod/42/8 HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host='localhost', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3aae71",
   "metadata": {},
   "source": [
    "http://localhost:8080/mul/6/7\n",
    "\n",
    "http://localhost:8080/div/42/8\n",
    "\n",
    "http://localhost:8080/mod/42/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba86e6",
   "metadata": {},
   "source": [
    "- Write a Python program that will test the web API made available through the requests\n",
    "library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9fd5f",
   "metadata": {},
   "source": [
    "Answer"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
