{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294988a5",
   "metadata": {},
   "source": [
    "# PROGRES - TME2\n",
    "\n",
    "Fabien Mathieu - fabien.mathieu@normalesup.org\n",
    "\n",
    "Sébastien Tixeuil - Sebastien.Tixeuil@lip6.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e89a39",
   "metadata": {},
   "source": [
    "# Students\n",
    "Student 1: Hongming FANG\n",
    "Student 2: Graham Preston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9046b32",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "- Star exercises (indicated by *) should only be done if all other exercises have been completed. You \n",
    "don't have to do them if you do not want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b0fc5",
   "metadata": {},
   "source": [
    "# Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f73082",
   "metadata": {},
   "source": [
    "1. Cite your sources\n",
    "2. One file to rule them all\n",
    "3. Explain\n",
    "4. Execute your code\n",
    "\n",
    "\n",
    "https://github.com/balouf/progres/blob/main/rules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a42a0",
   "metadata": {},
   "source": [
    "# Exercice 1 - Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777c51e",
   "metadata": {},
   "source": [
    "Consider the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd42208b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.166734Z",
     "start_time": "2024-10-11T07:03:31.162693Z"
    }
   },
   "outputs": [],
   "source": [
    "L = ['marie.Dupond@gmail.com', 'lucie.Durand@wanadoo.fr',\n",
    "'Sophie.Parmentier @@ gmail.com', 'franck.Dupres.gmail.com',\n",
    "'pierre.Martin@lip6 .fr ',' eric.Deschamps@gmail.com ']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522f37b",
   "metadata": {},
   "source": [
    "- Which of these entries are valid?\n",
    "- Use regular expressions to identify valid *gmail* addresses and display them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031d815",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f803bc",
   "metadata": {},
   "source": [
    "The valid entries are `'marie.Dupond@gmail.com'`, `' eric.Deschamps@gmail.com '`. We consider otherwise valid strings which are whitespace-padded to also be valid, as stripping is a simple operation, and this lends itself to a better user experience (if the user doesn't realize there is an invisible space, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58857cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.182237Z",
     "start_time": "2024-10-11T07:03:31.167743Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import functools\n",
    "from typing import List\n",
    "\n",
    "GMAIL_RE = re.compile(r'^\\s*([0-9A-Za-z_.]+@gmail.com)\\s*')\n",
    "\n",
    "def _true_gmail_reducer(accumulator: List[str], test_address: str) -> bool:\n",
    "    gmail_match = GMAIL_RE.match(test_address)\n",
    "    if not gmail_match: return accumulator\n",
    "    address = gmail_match.group(1)\n",
    "    return accumulator + [address]\n",
    "\n",
    "def true_gmail(mail_list: List[str]) -> List[str]:\n",
    "    return functools.reduce(_true_gmail_reducer, mail_list, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2496b3",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The `true_gmail` transforms a list of strings to a list of found, whitespace-stripped, gmail addresses. Because values of the output list may be transformed from those of the input list, a `reduce` is used in place of a `filter`. \n",
    "\n",
    "The reducer implements the logic. It tests against a gmail regex and implements two cases:\n",
    "1. If there is no match, throw out the address by returning the unchanged accumulator\n",
    "2. Otherwise, continue to the next iteration with the desired portion of the address, by returning the accumulator with the address portion appended\n",
    "\n",
    "Note that `+` is used for list extension rather than `.append`. This is to prevent any unexpected behavior that could come from mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "816fd798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.210273Z",
     "start_time": "2024-10-11T07:03:31.196497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marie.Dupond@gmail.com', 'eric.Deschamps@gmail.com']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_gmail(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375b8f3",
   "metadata": {},
   "source": [
    "- Use regular expressions to check if a string ends with a number. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c3615",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13acfb67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.221802Z",
     "start_time": "2024-10-11T07:03:31.211282Z"
    }
   },
   "outputs": [],
   "source": [
    "def ends_with_number(txt: str) -> bool:\n",
    "    return bool(re.match(r'^.*\\d$', txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef169126",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "`ends_with_number` checks for a match of the given parameter against the regular expression `^.*\\d$`. The regular expression could be worded in English as: \"match anything from the beginning of the string, then match a number followed by the string end\".\n",
    "\n",
    "`re.match` returns a `Match` object if a match is present, and `None` otherwise, but `ends_with_number` wants to return a boolean indicating yes or no. The result of `re.match` is transformed to the desired output by simply being passed to `bool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9da640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.246076Z",
     "start_time": "2024-10-11T07:03:31.235381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ends_with_number('to42to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa5f293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.257782Z",
     "start_time": "2024-10-11T07:03:31.248084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ends_with_number('to42to666')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bde5d7",
   "metadata": {},
   "source": [
    "- Use regular expressions to remove problematic zeros from an IPv4 address expressed as a \n",
    "string. (example: \"216.08.094.196\" should become \"216.8.94.196\", but \"216.80.140.196\" \n",
    "should remain \"216.80.140.196\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f606b5",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d2f77ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.271323Z",
     "start_time": "2024-10-11T07:03:31.261798Z"
    }
   },
   "outputs": [],
   "source": [
    "IPV4_FIELD_RE = re.compile(r'0*(\\d{1,3})')\n",
    "\n",
    "def normalize_ip(txt):\n",
    "    return '.'.join(IPV4_FIELD_RE.findall(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f5056",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "`normalize_ip` uses a regular expression to match the desired substring for each sequence within an IPv4 address. The list of desired sequences is taken using `.findall`, which is then re-formatted to an IPv4 string using `'.'.join`. \n",
    "\n",
    "The regular expression used is `0*(\\d{1,3})`. There are two parts to this expression:\n",
    "1. `0*` matches 0 or more of the character `0`, at the beginning of the sequence, outside the capture group\n",
    "2. `(\\d{1,3})` matches 1-3 digits in a row for a sequence, and puts them in a capture group\n",
    "\n",
    "The first part enables excluding leading `0`s from the capture group, while not requiring leading `0`s to match. The second part matching at least 1 digit enables capturing a `0` if it is the actual value of the sequence. e.g: The edge case `'000'` matches only the last `0` within the capture group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088d349e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.295246Z",
     "start_time": "2024-10-11T07:03:31.284633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'216.0.94.196'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_ip(\"216.0.094.196\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57c2cb4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.309262Z",
     "start_time": "2024-10-11T07:03:31.296255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'216.8.94.196'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_ip(\"216.08.094.196\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b6be3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.350525Z",
     "start_time": "2024-10-11T07:03:31.313271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'216.80.140.196'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_ip(\"216.80.140.196\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ac9d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.0.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_ip(\"000.00.0.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defac980",
   "metadata": {},
   "source": [
    "- Use regular expressions to transform a date from MM-DD-YYYY format to DD-MM-YYYY \n",
    "format. (example \"11-06-2020\" should become \"06-11-2020\"). Optionally*, do the same thing using the `datetime` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c7974",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01892f64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.360858Z",
     "start_time": "2024-10-11T07:03:31.356546Z"
    }
   },
   "outputs": [],
   "source": [
    "DATE_RE = re.compile(r'^(\\d{2})-(\\d{2})-(\\d{4})$')\n",
    "\n",
    "def switch_md(txt: str) -> str:\n",
    "    mm, dd, yyyy = DATE_RE.match(txt).groups()\n",
    "    return '-'.join([dd, mm, yyyy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c93b73",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "`switch_md` uses a regex to match a full date string and grab groups of each section, then re-orders and re-joins them to the desired format.\n",
    "\n",
    "Note that it is assumed the `txt` parameter matches this format, and does not define behavior for when this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f731cbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.399470Z",
     "start_time": "2024-10-11T07:03:31.386752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'06-11-2020'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_md(\"11-06-2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1a107",
   "metadata": {},
   "source": [
    "# Exercice 2 - Analyze XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a77fec",
   "metadata": {},
   "source": [
    "- Write a Python code that retrieves the content of the page at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a640a342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.410291Z",
     "start_time": "2024-10-11T07:03:31.400475Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.w3schools.com/xml/cd_catalog.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2670c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "s = Session()\n",
    "r = s.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ff873",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "To retrieve the URL content, `Sessions.get` is used, to give the option to keep cookies and re-use a TCP connection if we were making multiple requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371f1f2",
   "metadata": {},
   "source": [
    "- Look at the text content and load as xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8c5d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<CATALOG>\n",
      "  <CD>\n",
      "    <TITLE>Empire Burlesque</TITLE>\n",
      "    <ARTIST>Bob Dylan</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Columbia</COMPANY>\n",
      "    <PRICE>10.90</PRICE>\n",
      "    <YEAR>1985</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Hide your heart</TITLE>\n",
      "    <ARTIST>Bonnie Tyler</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>CBS Records</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1988</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Greatest Hits</TITLE>\n",
      "    <ARTIST>Dolly Parton</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>RCA</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1982</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Still got the blues</TITLE>\n",
      "    <ARTIST>Gary Moore</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Virgin records</COMPANY>\n",
      "    <PRICE>10.20</PRICE>\n",
      "    <YEAR>1990</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Eros</TITLE>\n",
      "    <ARTIST>Eros Ramazzotti</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>BMG</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1997</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>One night only</TITLE>\n",
      "    <ARTIST>Bee Gees</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Polydor</COMPANY>\n",
      "    <PRICE>10.90</PRICE>\n",
      "    <YEAR>1998</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Sylvias Mother</TITLE>\n",
      "    <ARTIST>Dr.Hook</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>CBS</COMPANY>\n",
      "    <PRICE>8.10</PRICE>\n",
      "    <YEAR>1973</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Maggie May</TITLE>\n",
      "    <ARTIST>Rod Stewart</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Pickwick</COMPANY>\n",
      "    <PRICE>8.50</PRICE>\n",
      "    <YEAR>1990</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Romanza</TITLE>\n",
      "    <ARTIST>Andrea Bocelli</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Polydor</COMPANY>\n",
      "    <PRICE>10.80</PRICE>\n",
      "    <YEAR>1996</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>When a man loves a woman</TITLE>\n",
      "    <ARTIST>Percy Sledge</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Atlantic</COMPANY>\n",
      "    <PRICE>8.70</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Black angel</TITLE>\n",
      "    <ARTIST>Savage Rose</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Mega</COMPANY>\n",
      "    <PRICE>10.90</PRICE>\n",
      "    <YEAR>1995</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>1999 Grammy Nominees</TITLE>\n",
      "    <ARTIST>Many</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Grammy</COMPANY>\n",
      "    <PRICE>10.20</PRICE>\n",
      "    <YEAR>1999</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>For the good times</TITLE>\n",
      "    <ARTIST>Kenny Rogers</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Mucik Master</COMPANY>\n",
      "    <PRICE>8.70</PRICE>\n",
      "    <YEAR>1995</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Big Willie style</TITLE>\n",
      "    <ARTIST>Will Smith</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Columbia</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1997</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Tupelo Honey</TITLE>\n",
      "    <ARTIST>Van Morrison</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Polydor</COMPANY>\n",
      "    <PRICE>8.20</PRICE>\n",
      "    <YEAR>1971</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Soulsville</TITLE>\n",
      "    <ARTIST>Jorn Hoel</ARTIST>\n",
      "    <COUNTRY>Norway</COUNTRY>\n",
      "    <COMPANY>WEA</COMPANY>\n",
      "    <PRICE>7.90</PRICE>\n",
      "    <YEAR>1996</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>The very best of</TITLE>\n",
      "    <ARTIST>Cat Stevens</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Island</COMPANY>\n",
      "    <PRICE>8.90</PRICE>\n",
      "    <YEAR>1990</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Stop</TITLE>\n",
      "    <ARTIST>Sam Brown</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>A and M</COMPANY>\n",
      "    <PRICE>8.90</PRICE>\n",
      "    <YEAR>1988</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Bridge of Spies</TITLE>\n",
      "    <ARTIST>T'Pau</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Siren</COMPANY>\n",
      "    <PRICE>7.90</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Private Dancer</TITLE>\n",
      "    <ARTIST>Tina Turner</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Capitol</COMPANY>\n",
      "    <PRICE>8.90</PRICE>\n",
      "    <YEAR>1983</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Midt om natten</TITLE>\n",
      "    <ARTIST>Kim Larsen</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Medley</COMPANY>\n",
      "    <PRICE>7.80</PRICE>\n",
      "    <YEAR>1983</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Pavarotti Gala Concert</TITLE>\n",
      "    <ARTIST>Luciano Pavarotti</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>DECCA</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1991</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>The dock of the bay</TITLE>\n",
      "    <ARTIST>Otis Redding</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Stax Records</COMPANY>\n",
      "    <PRICE>7.90</PRICE>\n",
      "    <YEAR>1968</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Picture book</TITLE>\n",
      "    <ARTIST>Simply Red</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Elektra</COMPANY>\n",
      "    <PRICE>7.20</PRICE>\n",
      "    <YEAR>1985</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Red</TITLE>\n",
      "    <ARTIST>The Communards</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>London</COMPANY>\n",
      "    <PRICE>7.80</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Unchain my heart</TITLE>\n",
      "    <ARTIST>Joe Cocker</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>EMI</COMPANY>\n",
      "    <PRICE>8.20</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "</CATALOG>\n",
      "\n",
      "Main tag: CATALOG; main attributes: {}\n"
     ]
    }
   ],
   "source": [
    "print(r.text)\n",
    "cds = ET.fromstring(r.text)\n",
    "print(f\"Main tag: {cds.tag}; main attributes: {cds.attrib}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb1cdc",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "To load the result as XML, `ElementTree.fromstring` is used, for simplicity's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d26eb",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ed90b",
   "metadata": {},
   "source": [
    "- Write a `display_cd` function that displays (i.e. `print`), for a CD: title, artist, country, company, year.\n",
    "- Display all CDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c1c3b4",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ee85351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.730469Z",
     "start_time": "2024-10-11T07:03:31.712761Z"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_cd(cd: ET) -> None:\n",
    "    properties = [f'{child.tag}: {child.text}' for child in cd]\n",
    "    print(', '.join(properties))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934cb66",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The chosen format for displaying a CD is to display all child tags and their text content, separated by commas. This is done by first creating a list of tags + values with the desired format, and then utilizing `.join` to easily intersperse commas, and printing the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94c7d5",
   "metadata": {},
   "source": [
    "- Display all 1980s CDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f548d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: Empire Burlesque, ARTIST: Bob Dylan, COUNTRY: USA, COMPANY: Columbia, PRICE: 10.90, YEAR: 1985\n",
      "TITLE: Hide your heart, ARTIST: Bonnie Tyler, COUNTRY: UK, COMPANY: CBS Records, PRICE: 9.90, YEAR: 1988\n",
      "TITLE: Greatest Hits, ARTIST: Dolly Parton, COUNTRY: USA, COMPANY: RCA, PRICE: 9.90, YEAR: 1982\n",
      "TITLE: When a man loves a woman, ARTIST: Percy Sledge, COUNTRY: USA, COMPANY: Atlantic, PRICE: 8.70, YEAR: 1987\n",
      "TITLE: Stop, ARTIST: Sam Brown, COUNTRY: UK, COMPANY: A and M, PRICE: 8.90, YEAR: 1988\n",
      "TITLE: Bridge of Spies, ARTIST: T'Pau, COUNTRY: UK, COMPANY: Siren, PRICE: 7.90, YEAR: 1987\n",
      "TITLE: Private Dancer, ARTIST: Tina Turner, COUNTRY: UK, COMPANY: Capitol, PRICE: 8.90, YEAR: 1983\n",
      "TITLE: Midt om natten, ARTIST: Kim Larsen, COUNTRY: EU, COMPANY: Medley, PRICE: 7.80, YEAR: 1983\n",
      "TITLE: Picture book, ARTIST: Simply Red, COUNTRY: EU, COMPANY: Elektra, PRICE: 7.20, YEAR: 1985\n",
      "TITLE: Red, ARTIST: The Communards, COUNTRY: UK, COMPANY: London, PRICE: 7.80, YEAR: 1987\n",
      "TITLE: Unchain my heart, ARTIST: Joe Cocker, COUNTRY: USA, COMPANY: EMI, PRICE: 8.20, YEAR: 1987\n"
     ]
    }
   ],
   "source": [
    "for cd in cds.findall(\"CD\"):\n",
    "    year = cd.find(\"YEAR\").text\n",
    "    if year and 1980 <= int(year) <= 1989:  \n",
    "        display_cd(cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73709331",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The root element has CDs as sub-elements. Since `display_cd` expects a single CD record, we iterate through the root and pass each child to `display_cd`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d207ba3",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395138d",
   "metadata": {},
   "source": [
    "- Display all British CDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d7efe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: Hide your heart, ARTIST: Bonnie Tyler, COUNTRY: UK, COMPANY: CBS Records, PRICE: 9.90, YEAR: 1988\n",
      "TITLE: Still got the blues, ARTIST: Gary Moore, COUNTRY: UK, COMPANY: Virgin records, PRICE: 10.20, YEAR: 1990\n",
      "TITLE: One night only, ARTIST: Bee Gees, COUNTRY: UK, COMPANY: Polydor, PRICE: 10.90, YEAR: 1998\n",
      "TITLE: Sylvias Mother, ARTIST: Dr.Hook, COUNTRY: UK, COMPANY: CBS, PRICE: 8.10, YEAR: 1973\n",
      "TITLE: Maggie May, ARTIST: Rod Stewart, COUNTRY: UK, COMPANY: Pickwick, PRICE: 8.50, YEAR: 1990\n",
      "TITLE: For the good times, ARTIST: Kenny Rogers, COUNTRY: UK, COMPANY: Mucik Master, PRICE: 8.70, YEAR: 1995\n",
      "TITLE: Tupelo Honey, ARTIST: Van Morrison, COUNTRY: UK, COMPANY: Polydor, PRICE: 8.20, YEAR: 1971\n",
      "TITLE: The very best of, ARTIST: Cat Stevens, COUNTRY: UK, COMPANY: Island, PRICE: 8.90, YEAR: 1990\n",
      "TITLE: Stop, ARTIST: Sam Brown, COUNTRY: UK, COMPANY: A and M, PRICE: 8.90, YEAR: 1988\n",
      "TITLE: Bridge of Spies, ARTIST: T'Pau, COUNTRY: UK, COMPANY: Siren, PRICE: 7.90, YEAR: 1987\n",
      "TITLE: Private Dancer, ARTIST: Tina Turner, COUNTRY: UK, COMPANY: Capitol, PRICE: 8.90, YEAR: 1983\n",
      "TITLE: Pavarotti Gala Concert, ARTIST: Luciano Pavarotti, COUNTRY: UK, COMPANY: DECCA, PRICE: 9.90, YEAR: 1991\n",
      "TITLE: Red, ARTIST: The Communards, COUNTRY: UK, COMPANY: London, PRICE: 7.80, YEAR: 1987\n"
     ]
    }
   ],
   "source": [
    "british_cds = cds.findall(\"CD[COUNTRY='UK']\")\n",
    "for bcd in british_cds:\n",
    "  display_cd(bcd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a05f0b",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "This code uses XPath to find all British CDs. It does this by selecting all `CD` tags which have a sub-tag `COUNTRY` with the text value `UK`.\n",
    "\n",
    "Reference: [XPath section of the ElementTree docs](https://docs.python.org/3/library/xml.etree.elementtree.html#xpath-support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63660b",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda5235",
   "metadata": {},
   "source": [
    "# Exercice 3 - Analyze JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b24d30",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Write a Python program that gets the file of filming locations in Paris at: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de719209",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.786738Z",
     "start_time": "2024-10-11T07:03:31.778307Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://opendata.paris.fr/explore/dataset/lieux-de-tournage-a-paris/download/?format=json&timezone=Europe/Berlin&lang=fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343174e2",
   "metadata": {},
   "source": [
    "- How many entries have you got?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3e1f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuboshu/opt/anaconda3/envs/bima/lib/python3.12/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'opendata.paris.fr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry count: 12265\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def download(source_url, dest_file):\n",
    "  s = Session()\n",
    "  s.verify = False\n",
    "  r = s.get(source_url, stream=True)\n",
    "  dest_file = Path(dest_file)\n",
    "\n",
    "  with open(dest_file, 'wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size=8192):\n",
    "      if chunk:\n",
    "        f.write(chunk)\n",
    "\n",
    "FN = 'tournage.json'\n",
    "download(url, FN)\n",
    "\n",
    "with open(FN) as f:\n",
    "  locs = json.load(f)\n",
    "\n",
    "print('Entry count:', len(locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378352d",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "This code makes use of the sample `download` function from the slides. The JSON file is downloaded to `tournage.json`, which is then re-opened to analyze. Since there is an array at the root, `len` is simply called on the loaded JSON to get the entry count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1eca6",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d23f6",
   "metadata": {},
   "source": [
    "- Analyze the JSON file: what is its structure?\n",
    "- Write a function that converts an entry in a string that shows director, title, district, start date, end date, and geographic coordinates.\n",
    "- Convert all entries in strings (warning: some entries may have issues).\n",
    "- Display the first 20 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f051f79",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f48e3fd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.576923Z",
     "start_time": "2024-10-11T07:03:35.572252Z"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_loc(entry):\n",
    "    fields = entry['fields']\n",
    "    director = fields.get('nom_realisateur', '<director missing>')\n",
    "    title = fields.get('nom_tournage', '<title missing>')\n",
    "    district = fields.get('ardt_lieu', '<district missing>')\n",
    "    start_date = fields.get('date_debut', '<start date missing>')\n",
    "    end_date = fields.get('date_fin', '<end date missing>')\n",
    "    coord_x = fields.get('coord_x', '<x coordinate missing>')\n",
    "    coord_y = fields.get('coord_y', '<y coordinate missing>')\n",
    "\n",
    "    return f\"{director}'s \\\"{title},\\\" filmed in {district} ({coord_x}, {coord_y}) from {start_date} to {end_date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb5f54",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "Metadata for each entry is stored in the `'fields'` key, however there may be missing fields for each entry. To safeguard for this, `dict.get` is used to give a default value in the case of a missing key.\n",
    "\n",
    "### File structure\n",
    "\n",
    "The JSON structure is an array of entries. The following is a formatted entry, to give an example of real data:\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"datasetid\":\"lieux-de-tournage-a-paris\",\n",
    "   \"recordid\":\"0ff321c5b140a12a8e50a1b212a7c5f5bced91d7\",\n",
    "   \"fields\":{\n",
    "      \"coord_x\":2.37006242,\n",
    "      \"id_lieu\":\"2017-751\",\n",
    "      \"adresse_lieu\":\"rue du faubourg du temple, 75011 paris\",\n",
    "      \"geo_shape\":{\n",
    "         \"coordinates\":[\n",
    "            2.370062415669748,\n",
    "            48.8696979988026\n",
    "         ],\n",
    "         \"type\":\"Point\"\n",
    "      },\n",
    "      \"coord_y\":48.869698,\n",
    "      \"ardt_lieu\":\"75011\",\n",
    "      \"nom_tournage\":\"2 Fils (Nouvelle Demande Décor Librairie / Journées interverties)\",\n",
    "      \"nom_realisateur\":\"Félix MOATI\",\n",
    "      \"date_debut\":\"2017-10-19\",\n",
    "      \"type_tournage\":\"Long métrage\",\n",
    "      \"annee_tournage\":\"2017\",\n",
    "      \"nom_producteur\":\"NORD OUEST FILMS\",\n",
    "      \"date_fin\":\"2017-10-19\",\n",
    "      \"geo_point_2d\":[\n",
    "         48.8696979988026,\n",
    "         2.370062415669748\n",
    "      ]\n",
    "   },\n",
    "   \"geometry\":{\n",
    "      \"type\":\"Point\",\n",
    "      \"coordinates\":[\n",
    "         2.370062415669748,\n",
    "         48.8696979988026\n",
    "      ]\n",
    "   },\n",
    "   \"record_timestamp\":\"2024-01-31T13:40:46.402+01:00\"\n",
    "}\n",
    "```\n",
    "\n",
    "Each entry may be missing specific keys from `\"fields\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f86ee18d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.670903Z",
     "start_time": "2024-10-11T07:03:35.614408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNE FONTAINE's \"POLICE,\" filmed in 75012 (2.39934074, 48.83798025) from 2019-03-08 to 2019-03-09\n",
      "Eli Ben-David's \"L'Attaché,\" filmed in 75018 (2.34443461, 48.88730126) from 2019-03-14 to 2019-03-14\n",
      "Marc RECUENCO's \"En attendant qui ? Mai,\" filmed in 75017 (2.30595278, 48.8835646) from 2019-06-11 to 2019-06-11\n",
      "JEAN PASCAL ZADI ET JOHN WAXXX's \"TOUT SIMPLEMENT NOIR,\" filmed in 75005 (2.35024547, 48.84859142) from 2019-05-23 to 2019-05-23\n",
      "Nicolas Herdt's \"Une famille formidable,\" filmed in 75003 (2.36365029, 48.8602504) from 2018-08-06 to 2018-08-06\n",
      "Nicolas Herdt's \"Une famille formidable,\" filmed in 75003 (2.3621555, 48.86295435) from 2018-08-06 to 2018-08-06\n",
      "Maïmouna Doucouré's \"Les Mignonnes,\" filmed in 75019 (2.38208807, 48.88213499) from 2018-08-07 to 2018-08-07\n",
      "CHRISTOPHE BARRAUD's \"LEBOWITZ CONTRE LEBOWITZ/9 A 12,\" filmed in 75013 (2.359355, 48.838779) from 2016-11-09 to 2016-11-09\n",
      "NICOLAS HERDT's \"LEO MATTEI/14 ET 15,\" filmed in 75004 (2.365669, 48.84726) from 2016-10-06 to 2016-10-06\n",
      "JEANNE HERRY's \"10%/SAISON 2,\" filmed in 75019 (2.384688, 48.873777) from 2016-09-14 to 2016-09-14\n",
      "DAVID MICHOD's \"WAR MACHINE,\" filmed in 75001 (2.338081, 48.861863) from 2016-01-30 to 2016-01-30\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75013 (2.35076218, 48.8288281) from 2020-12-22 to 2020-12-22\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75013 (2.35137819, 48.82793218) from 2020-12-22 to 2020-12-22\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75013 (2.35137819, 48.82793218) from 2020-12-22 to 2020-12-22\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75001 (2.34748746, 48.86046195) from 2020-12-22 to 2020-12-22\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" filmed in 75001 (2.34772227, 48.86094047) from 2020-12-22 to 2020-12-22\n",
      "Roman Polanski's \"J'ACCUSE,\" filmed in 75015 (2.30996607, 48.84698001) from 2019-02-25 to 2019-02-25\n",
      "Alexandre Laurent's \"Le Bazar de la Charité,\" filmed in 75008 (2.31245551, 48.87963777) from 2019-03-14 to 2019-03-15\n",
      "Alexandre Laurent's \"Le Bazar de la Charité,\" filmed in 75008 (2.31123909, 48.88051652) from 2019-03-18 to 2019-03-18\n",
      "Maxime Roy's \"LES HEROIQUES,\" filmed in 75010 (2.36143507, 48.88235204) from 2019-03-11 to 2019-03-12\n"
     ]
    }
   ],
   "source": [
    "all_entries = [display_loc(e) for e in locs]\n",
    "print('\\n'.join(all_entries[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bb085",
   "metadata": {},
   "source": [
    "- A same movie can have multiple shooting locations. Make a list of movies, where each entry contains the movie title, its director, and shootings locations (district, start date, end date).\n",
    "- How many movies do you have?\n",
    "- Write a function that converts a movie into a string that shows director, title, and shootings.\n",
    "- Convert all movies in strings.\n",
    "- Display the first 20 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023a234",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f65854ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.675898Z",
     "start_time": "2024-10-11T07:03:35.672816Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, TypeVar, List\n",
    "\n",
    "Movie = TypeVar('Movie')\n",
    "movies: Dict[str, Movie] = dict()\n",
    "\n",
    "for loc in locs:\n",
    "  title = loc['fields']['nom_tournage']\n",
    "  if title not in movies:\n",
    "    movies[title] = {\n",
    "      'title': title,\n",
    "      'director': loc['fields'].get('nom_realisateur', '<director missing>'),\n",
    "      'shootings': []\n",
    "    }\n",
    "  movies[title]['shootings'].append({\n",
    "    'district': loc['fields'].get('ardt_lieu', '<arrondissement missing>'),\n",
    "    'start_date': loc['fields']['date_debut'],\n",
    "    'end_date': loc['fields']['date_fin']\n",
    "  })\n",
    "\n",
    "# Regroup locations per movie\n",
    "movies: List[Movie] = [m for m in movies.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d017c",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The question asks for two tasks to be accomplished:\n",
    "1. Entries are grouped by which movie they are a part of\n",
    "2. A subset of fields is displayed from each movie, including the newly aggregated field of shooting locations\n",
    "\n",
    "The most straightforward way to create this aggregation is via a dictionary. The movie title is chosen as the key, as there are no better unique identifier fields referencing the movie itself. \n",
    "\n",
    "While this organization is being done, the opportunity is taken to normalize the data into a new structure containing exactly what we need, and with no fields missing:\n",
    "\n",
    "```json\n",
    "A Movie is a dictionary with the schema:\n",
    "\n",
    "{\n",
    "  \"title\": \"string\",\n",
    "  \"director\": \"string\",\n",
    "  \"shootings\": [\n",
    "    {\n",
    "      \"district\": \"string\",\n",
    "      \"start_date\": \"string\",\n",
    "      \"end_date\": \"string\"\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Since the top-level dictionary was only needed for the process of organization, and not for the final data representation, we re-organize all of its values into a list for the final `movies` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cb17c8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.728960Z",
     "start_time": "2024-10-11T07:03:35.721811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1476"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c07db010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.740289Z",
     "start_time": "2024-10-11T07:03:35.730968Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def display_movie(movie):\n",
    "    movie_str = f\"{movie['director']}'s \\\"{movie['title']},\\\" was filmed in the following locations:\\n\"\n",
    "    for shooting in movie['shootings']:\n",
    "        movie_str += f'- {shooting['district']} between {shooting['start_date']} and {shooting['end_date']}\\n'\n",
    "    return movie_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5e8374d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.773580Z",
     "start_time": "2024-10-11T07:03:35.753086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNE FONTAINE's \"POLICE,\" was filmed in the following locations:\n",
      "- 75012 between 2019-03-08 and 2019-03-09\n",
      "- 75012 between 2019-03-08 and 2019-03-09\n",
      "- 75012 between 2019-04-10 and 2019-04-11\n",
      "- 75012 between 2019-03-11 and 2019-03-12\n",
      "- 75020 between 2019-03-27 and 2019-03-27\n",
      "- 75012 between 2019-03-07 and 2019-03-08\n",
      "- 75011 between 2019-03-27 and 2019-03-27\n",
      "- 75019 between 2019-03-28 and 2019-03-28\n",
      "- 75012 between 2019-03-25 and 2019-03-25\n",
      "- 75012 between 2019-03-28 and 2019-03-28\n",
      "- 75019 between 2019-04-08 and 2019-04-09\n",
      "\n",
      "Eli Ben-David's \"L'Attaché,\" was filmed in the following locations:\n",
      "- 75018 between 2019-03-14 and 2019-03-14\n",
      "- 75018 between 2019-03-14 and 2019-03-14\n",
      "- 75005 between 2019-03-15 and 2019-03-15\n",
      "- 75012 between 2019-03-12 and 2019-03-12\n",
      "- 75009 between 2019-03-12 and 2019-03-12\n",
      "- 75001 between 2019-03-12 and 2019-03-12\n",
      "- 75004 between 2019-03-20 and 2019-03-20\n",
      "- 75001 between 2019-03-15 and 2019-03-16\n",
      "- 75004 between 2019-03-16 and 2019-03-16\n",
      "- 75005 between 2019-03-12 and 2019-03-12\n",
      "- 75010 between 2019-03-12 and 2019-03-12\n",
      "- 75007 between 2019-03-13 and 2019-03-13\n",
      "- 75010 between 2019-03-12 and 2019-03-12\n",
      "- 75004 between 2019-03-16 and 2019-03-16\n",
      "- 75018 between 2019-03-14 and 2019-03-15\n",
      "- 75003 between 2019-03-16 and 2019-03-16\n",
      "- 75018 between 2019-03-14 and 2019-03-14\n",
      "- 75001 between 2019-03-21 and 2019-03-22\n",
      "- 75004 between 2019-03-22 and 2019-03-23\n",
      "\n",
      "Marc RECUENCO's \"En attendant qui ? Mai,\" was filmed in the following locations:\n",
      "- 75017 between 2019-06-11 and 2019-06-11\n",
      "- 75017 between 2019-06-10 and 2019-06-10\n",
      "- 75017 between 2019-06-15 and 2019-06-15\n",
      "- 75017 between 2019-06-22 and 2019-06-23\n",
      "- 75005 between 2019-06-17 and 2019-06-18\n",
      "- 75005 between 2019-06-23 and 2019-06-23\n",
      "- 75017 between 2019-06-13 and 2019-06-13\n",
      "- 75017 between 2019-06-06 and 2019-06-07\n",
      "- 75013 between 2019-06-22 and 2019-06-22\n",
      "- 75017 between 2019-06-15 and 2019-06-15\n",
      "- 75017 between 2019-06-22 and 2019-06-23\n",
      "- 75017 between 2019-06-11 and 2019-06-11\n",
      "- 75017 between 2019-06-07 and 2019-06-07\n",
      "- 75017 between 2019-06-11 and 2019-06-11\n",
      "- 75017 between 2019-06-22 and 2019-06-23\n",
      "- 75005 between 2019-06-17 and 2019-06-17\n",
      "\n",
      "JEAN PASCAL ZADI ET JOHN WAXXX's \"TOUT SIMPLEMENT NOIR,\" was filmed in the following locations:\n",
      "- 75005 between 2019-05-23 and 2019-05-23\n",
      "- 75008 between 2019-06-12 and 2019-06-12\n",
      "- 75010 between 2019-05-30 and 2019-05-30\n",
      "- 75019 between 2019-06-26 and 2019-06-26\n",
      "- 75008 between 2019-06-13 and 2019-06-13\n",
      "- 75020 between 2019-06-04 and 2019-06-04\n",
      "- 75001 between 2019-06-28 and 2019-06-28\n",
      "- 75015 between 2019-07-05 and 2019-07-05\n",
      "- 75007 between 2019-07-05 and 2019-07-05\n",
      "- 75002 between 2019-06-16 and 2019-06-17\n",
      "- 75020 between 2019-06-04 and 2019-06-04\n",
      "- 75007 between 2019-05-30 and 2019-05-31\n",
      "- 75002 between 2019-06-26 and 2019-06-26\n",
      "- 75009 between 2019-06-24 and 2019-06-24\n",
      "- 75011 between 2019-07-10 and 2019-07-10\n",
      "- 75007 between 2019-07-05 and 2019-07-05\n",
      "- 75007 between 2019-05-31 and 2019-05-31\n",
      "- 75001 between 2019-06-26 and 2019-06-26\n",
      "- 75015 between 2019-06-03 and 2019-06-03\n",
      "- 75015 between 2019-07-05 and 2019-07-05\n",
      "- 75002 between 2019-06-12 and 2019-06-12\n",
      "- 75017 between 2019-06-25 and 2019-06-25\n",
      "- 75006 between 2019-07-10 and 2019-07-11\n",
      "- 75007 between 2019-07-05 and 2019-07-05\n",
      "- 75005 between 2019-05-23 and 2019-05-23\n",
      "- 75009 between 2019-05-22 and 2019-05-23\n",
      "- 75008 between 2019-06-28 and 2019-06-28\n",
      "- 75004 between 2019-05-23 and 2019-05-23\n",
      "- 75002 between 2019-06-24 and 2019-06-24\n",
      "- 75116 between 2019-06-28 and 2019-06-28\n",
      "- 75010 between 2019-05-29 and 2019-05-30\n",
      "- 75002 between 2019-06-12 and 2019-06-12\n",
      "- 75002 between 2019-06-24 and 2019-06-24\n",
      "- 75001 between 2019-06-24 and 2019-06-24\n",
      "- 75007 between 2019-05-31 and 2019-05-31\n",
      "- 75004 between 2019-05-20 and 2019-05-20\n",
      "- 75009 between 2019-06-12 and 2019-06-12\n",
      "\n",
      "Nicolas Herdt's \"Une famille formidable,\" was filmed in the following locations:\n",
      "- 75003 between 2018-08-06 and 2018-08-06\n",
      "- 75003 between 2018-08-06 and 2018-08-06\n",
      "- 75007 between 2018-08-13 and 2018-08-13\n",
      "- 75013 between 2018-08-24 and 2018-08-24\n",
      "- 75006 between 2018-08-31 and 2018-08-31\n",
      "- 75006 between 2018-08-31 and 2018-08-31\n",
      "- 75003 between 2018-08-07 and 2018-08-08\n",
      "- 75007 between 2018-08-13 and 2018-08-13\n",
      "\n",
      "Maïmouna Doucouré's \"Les Mignonnes,\" was filmed in the following locations:\n",
      "- 75019 between 2018-08-07 and 2018-08-07\n",
      "- 75019 between 2018-08-22 and 2018-08-27\n",
      "- 75019 between 2018-08-06 and 2018-08-06\n",
      "- 75019 between 2018-08-06 and 2018-08-06\n",
      "- 75019 between 2018-08-06 and 2018-08-06\n",
      "- 75019 between 2018-08-08 and 2018-08-08\n",
      "- 75019 between 2018-08-08 and 2018-08-08\n",
      "- 75019 between 2018-08-22 and 2018-08-27\n",
      "\n",
      "CHRISTOPHE BARRAUD's \"LEBOWITZ CONTRE LEBOWITZ/9 A 12,\" was filmed in the following locations:\n",
      "- 75013 between 2016-11-09 and 2016-11-09\n",
      "- 75018 between 2016-11-04 and 2016-11-04\n",
      "- 75011 between 2016-10-31 and 2016-10-31\n",
      "- 75002 between 2016-11-01 and 2016-11-01\n",
      "- 75018 between 2016-11-04 and 2016-11-04\n",
      "- 75011 between 2016-11-02 and 2016-11-02\n",
      "- 75013 between 2016-11-09 and 2016-11-09\n",
      "- 75011 between 2016-10-31 and 2016-10-31\n",
      "- 75010 between 2016-10-27 and 2016-10-27\n",
      "- 75010 between 2016-10-27 and 2016-10-27\n",
      "- 75010 between 2016-10-27 and 2016-10-27\n",
      "- 75004 between 2016-10-24 and 2016-10-24\n",
      "- 75016 between 2016-10-25 and 2016-10-26\n",
      "- 75018 between 2016-11-04 and 2016-11-04\n",
      "- 75018 between 2016-11-04 and 2016-11-04\n",
      "\n",
      "NICOLAS HERDT's \"LEO MATTEI/14 ET 15,\" was filmed in the following locations:\n",
      "- 75004 between 2016-10-06 and 2016-10-06\n",
      "- 75003 between 2016-10-05 and 2016-10-05\n",
      "- 75004 between 2016-10-06 and 2016-10-06\n",
      "- 75003 between 2016-10-05 and 2016-10-05\n",
      "- 75010 between 2016-10-18 and 2016-10-18\n",
      "- 75010 between 2016-10-18 and 2016-10-18\n",
      "- 75003 between 2016-10-05 and 2016-10-05\n",
      "- 75012 between 2016-10-07 and 2016-10-07\n",
      "- 75009 between 2016-10-01 and 2016-10-01\n",
      "- 75004 between 2016-10-06 and 2016-10-06\n",
      "\n",
      "JEANNE HERRY's \"10%/SAISON 2,\" was filmed in the following locations:\n",
      "- 75019 between 2016-09-14 and 2016-09-14\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75008 between 2016-09-12 and 2016-09-12\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75008 between 2016-09-13 and 2016-09-13\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75008 between 2016-10-17 and 2016-10-18\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75008 between 2016-10-14 and 2016-10-14\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75002 between 2016-10-03 and 2016-10-03\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75020 between 2016-09-14 and 2016-09-14\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75016 between 2016-10-10 and 2016-10-10\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75019 between 2016-09-16 and 2016-09-16\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75019 between 2016-10-03 and 2016-10-03\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75008 between 2016-10-11 and 2016-10-11\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75002 between 2016-09-19 and 2016-09-19\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75016 between 2016-10-05 and 2016-10-05\n",
      "- 75008 between 2016-09-13 and 2016-09-13\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75001 between 2016-09-05 and 2016-09-05\n",
      "- 75016 between 2016-10-13 and 2016-10-13\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75016 between 2016-10-06 and 2016-10-06\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75016 between 2016-09-15 and 2016-09-15\n",
      "- 75009 between 2016-09-29 and 2016-09-29\n",
      "- 75019 between 2016-09-16 and 2016-09-16\n",
      "- 75007 between 2016-10-10 and 2016-10-10\n",
      "- 75009 between 2016-09-06 and 2016-09-06\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "- 75020 between 2016-09-30 and 2016-09-30\n",
      "\n",
      "DAVID MICHOD's \"WAR MACHINE,\" was filmed in the following locations:\n",
      "- 75001 between 2016-01-30 and 2016-01-30\n",
      "- 75009 between 2016-01-30 and 2016-01-30\n",
      "- 75001 between 2016-01-31 and 2016-01-31\n",
      "- 75009 between 2016-01-31 and 2016-01-31\n",
      "- 75001 between 2016-01-31 and 2016-01-31\n",
      "- 75009 between 2016-01-31 and 2016-01-31\n",
      "- 75008 between 2016-01-31 and 2016-01-31\n",
      "- 75001 between 2016-01-31 and 2016-01-31\n",
      "- 75007 between 2016-01-30 and 2016-01-30\n",
      "\n",
      "Sylvie Verheyde's \"STELLA EST AMOUREUSE,\" was filmed in the following locations:\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2021-02-24 and 2021-02-24\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75003 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-18 and 2021-02-18\n",
      "- 75013 between 2021-02-22 and 2021-02-22\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75003 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2021-02-11 and 2021-02-12\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75001 between 2021-02-15 and 2021-02-15\n",
      "- 75005 between 2021-02-24 and 2021-02-24\n",
      "- 75013 between 2021-02-25 and 2021-02-26\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2021-02-11 and 2021-02-12\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75003 between 2021-02-09 and 2021-02-10\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75012 between 2021-02-25 and 2021-02-25\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-18 and 2021-02-18\n",
      "- 75005 between 2021-02-24 and 2021-02-24\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75001 between 2021-02-26 and 2021-02-26\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2021-02-10 and 2021-02-11\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75013 between 2021-02-22 and 2021-02-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75005 between 2021-02-24 and 2021-02-24\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75003 between 2021-02-12 and 2021-02-13\n",
      "- 75013 between 2021-02-16 and 2021-02-16\n",
      "- 75001 between 2021-02-15 and 2021-02-15\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75005 between 2021-02-08 and 2021-02-08\n",
      "- 75001 between 2021-02-26 and 2021-02-26\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2020-12-22 and 2020-12-22\n",
      "- 75001 between 2020-12-22 and 2020-12-22\n",
      "- 75013 between 2021-02-23 and 2021-02-23\n",
      "\n",
      "Roman Polanski's \"J'ACCUSE,\" was filmed in the following locations:\n",
      "- 75015 between 2019-02-25 and 2019-02-25\n",
      "- 75009 between 2019-02-13 and 2019-02-15\n",
      "- 75009 between 2019-01-31 and 2019-02-01\n",
      "- 75009 between 2019-02-13 and 2019-02-15\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75015 between 2019-02-26 and 2019-02-27\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75015 between 2019-02-22 and 2019-02-22\n",
      "- 75001 between 2019-01-29 and 2019-01-29\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75016 between 2019-02-20 and 2019-02-21\n",
      "- 75006 between 2019-03-04 and 2019-03-04\n",
      "- 75015 between 2019-02-21 and 2019-02-22\n",
      "\n",
      "Alexandre Laurent's \"Le Bazar de la Charité,\" was filmed in the following locations:\n",
      "- 75008 between 2019-03-14 and 2019-03-15\n",
      "- 75008 between 2019-03-18 and 2019-03-18\n",
      "- 75016 between 2019-02-18 and 2019-02-24\n",
      "- 75004 between 2019-03-07 and 2019-03-07\n",
      "- 75016 between 2019-02-23 and 2019-02-27\n",
      "- 75004 between 2019-03-04 and 2019-03-05\n",
      "- 75004 between 2019-03-08 and 2019-03-08\n",
      "- 75008 between 2019-03-19 and 2019-03-19\n",
      "- 75008 between 2019-03-14 and 2019-03-16\n",
      "- 75016 between 2019-02-11 and 2019-02-19\n",
      "- 75008 between 2019-03-15 and 2019-03-16\n",
      "- 75008 between 2019-03-11 and 2019-03-16\n",
      "- 75004 between 2019-03-06 and 2019-03-06\n",
      "- 75008 between 2019-03-11 and 2019-03-12\n",
      "- 75005 between 2019-03-05 and 2019-03-06\n",
      "- 75004 between 2019-03-07 and 2019-03-07\n",
      "- 75005 between 2019-03-04 and 2019-03-04\n",
      "- 75008 between 2019-03-13 and 2019-03-13\n",
      "- 75005 between 2019-03-05 and 2019-03-05\n",
      "\n",
      "Maxime Roy's \"LES HEROIQUES,\" was filmed in the following locations:\n",
      "- 75010 between 2019-03-11 and 2019-03-12\n",
      "\n",
      "PHILIPPE LIORET's \"PARIS BREST,\" was filmed in the following locations:\n",
      "- 75006 between 2019-03-15 and 2019-03-15\n",
      "- 75006 between 2019-03-15 and 2019-03-15\n",
      "\n",
      "Damien Chazelle's \"The Eddy,\" was filmed in the following locations:\n",
      "- 75013 between 2019-05-28 and 2019-05-28\n",
      "- 75012 between 2019-05-28 and 2019-05-28\n",
      "- 75013 between 2019-05-15 and 2019-05-16\n",
      "- 75012 between 2019-05-28 and 2019-05-28\n",
      "- 75020 between 2019-10-11 and 2019-10-12\n",
      "- 75016 between 2019-05-24 and 2019-05-24\n",
      "- 75016 between 2019-09-02 and 2019-09-02\n",
      "- 75012 between 2019-06-17 and 2019-06-20\n",
      "- 75012 between 2019-08-21 and 2019-08-31\n",
      "- 75020 between 2019-08-20 and 2019-08-20\n",
      "- 75012 between 2019-10-03 and 2019-10-04\n",
      "- 75012 between 2019-09-13 and 2019-09-13\n",
      "- 75016 between 2019-05-24 and 2019-05-24\n",
      "- 75020 between 2019-05-16 and 2019-05-17\n",
      "- 75020 between 2019-09-30 and 2019-10-02\n",
      "- 75020 between 2019-08-14 and 2019-08-14\n",
      "- 75004 between 2019-08-19 and 2019-08-19\n",
      "- 75020 between 2019-04-11 and 2019-04-11\n",
      "- 75013 between 2019-05-28 and 2019-05-29\n",
      "- 75012 between 2019-05-31 and 2019-06-01\n",
      "- 75011 between 2019-08-20 and 2019-08-21\n",
      "- 75014 between 2019-05-22 and 2019-05-22\n",
      "- 75013 between 2019-05-28 and 2019-05-28\n",
      "- 75016 between 2019-08-09 and 2019-08-09\n",
      "- 75012 between 2019-06-11 and 2019-06-11\n",
      "- 75020 between 2019-05-07 and 2019-05-07\n",
      "- 75012 between 2019-09-18 and 2019-09-20\n",
      "- 75020 between 2019-09-30 and 2019-10-01\n",
      "- 75013 between 2019-05-28 and 2019-05-28\n",
      "- 75019 between 2019-05-07 and 2019-05-08\n",
      "- 75010 between 2019-08-20 and 2019-08-22\n",
      "- 75019 between 2019-06-24 and 2019-06-25\n",
      "- 75013 between 2019-05-27 and 2019-05-29\n",
      "- 75012 between 2019-05-28 and 2019-05-28\n",
      "- 75012 between 2019-06-04 and 2019-06-04\n",
      "- 75014 between 2019-08-06 and 2019-08-08\n",
      "- 75012 between 2019-07-15 and 2019-07-16\n",
      "- 75014 between 2019-05-23 and 2019-05-24\n",
      "- 75018 between 2019-06-28 and 2019-06-28\n",
      "- 75012 between 2019-06-05 and 2019-06-05\n",
      "- 75116 between 2019-09-02 and 2019-09-03\n",
      "- 75014 between 2019-07-16 and 2019-07-17\n",
      "- 75019 between 2019-08-20 and 2019-08-20\n",
      "- 75012 between 2019-09-17 and 2019-09-17\n",
      "- 75014 between 2019-05-23 and 2019-05-23\n",
      "- 75012 between 2019-09-12 and 2019-09-12\n",
      "- 75012 between 2019-09-17 and 2019-09-17\n",
      "- 75116 between 2019-09-24 and 2019-09-24\n",
      "- 75013 between 2019-05-27 and 2019-05-28\n",
      "- 75012 between 2019-05-30 and 2019-05-30\n",
      "- 75007 between 2019-04-30 and 2019-05-01\n",
      "- 75020 between 2019-05-07 and 2019-05-08\n",
      "- 75012 between 2019-05-29 and 2019-05-29\n",
      "- 75016 between 2019-09-24 and 2019-09-24\n",
      "- 75014 between 2019-05-22 and 2019-05-24\n",
      "- 75019 between 2019-04-11 and 2019-04-11\n",
      "- 75010 between 2019-07-04 and 2019-07-04\n",
      "- 75012 between 2019-06-26 and 2019-06-26\n",
      "- 75002 between 2019-07-01 and 2019-07-02\n",
      "\n",
      "MARTIN PROVOST's \"LA SAGE FEMME,\" was filmed in the following locations:\n",
      "- 75008 between 2016-04-11 and 2016-04-11\n",
      "- 75008 between 2016-04-05 and 2016-04-06\n",
      "- 75018 between 2016-03-31 and 2016-03-31\n",
      "- 75005 between 2016-04-13 and 2016-04-13\n",
      "- 75018 between 2016-05-24 and 2016-05-24\n",
      "- 75008 between 2016-04-11 and 2016-04-11\n",
      "- 75018 between 2016-03-31 and 2016-03-31\n",
      "- 75018 between 2016-05-24 and 2016-05-24\n",
      "- 75016 between 2016-05-20 and 2016-05-20\n",
      "- 75018 between 2016-05-24 and 2016-05-24\n",
      "- 75005 between 2016-04-12 and 2016-04-12\n",
      "- 75008 between 2016-04-11 and 2016-04-11\n",
      "- 75018 between 2016-05-24 and 2016-05-24\n",
      "- 75008 between 2016-04-11 and 2016-04-11\n",
      "- 75005 between 2016-04-13 and 2016-04-13\n",
      "- 75016 between 2016-05-04 and 2016-05-04\n",
      "- 75008 between 2016-04-07 and 2016-04-07\n",
      "\n",
      "Jim Bagdonas's \"MODERN FAMILY 11,\" was filmed in the following locations:\n",
      "- 75005 between 2019-11-15 and 2019-11-15\n",
      "- 75004 between 2019-11-15 and 2019-11-15\n",
      "- 75004 between 2019-11-15 and 2019-11-15\n",
      "- 75005 between 2019-11-15 and 2019-11-15\n",
      "- 75004 between 2019-11-15 and 2019-11-15\n",
      "\n",
      "julien ZIDI's \"Alice NEVERS,\" was filmed in the following locations:\n",
      "- 75116 between 2019-11-19 and 2019-11-19\n",
      "- 75116 between 2019-11-21 and 2019-11-21\n",
      "- 75001 between 2019-11-16 and 2019-11-16\n",
      "- 75009 between 2018-11-20 and 2018-11-21\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75116 between 2019-11-22 and 2019-11-23\n",
      "- 75007 between 2019-11-25 and 2019-11-25\n",
      "- 75006 between 2018-11-19 and 2018-11-20\n",
      "- 75019 between 2019-11-29 and 2019-11-29\n",
      "- 75019 between 2019-11-28 and 2019-11-28\n",
      "- 75116 between 2019-11-18 and 2019-11-21\n",
      "- 75116 between 2018-11-16 and 2018-11-16\n",
      "- 75019 between 2019-11-29 and 2019-11-29\n",
      "- 75001 between 2019-11-13 and 2019-11-13\n",
      "- 75007 between 2018-11-26 and 2018-11-26\n",
      "- 75001 between 2019-11-16 and 2019-11-17\n",
      "- 75001 between 2019-11-16 and 2019-11-16\n",
      "- 75019 between 2019-11-28 and 2019-11-28\n",
      "\n",
      "Katia LEWKOWICZ's \"FORTE,\" was filmed in the following locations:\n",
      "- 75018 between 2018-12-07 and 2018-12-08\n",
      "- 75019 between 2018-12-05 and 2018-12-06\n",
      "- 75018 between 2018-11-08 and 2018-11-08\n",
      "- 75008 between 2018-11-13 and 2018-11-13\n",
      "- 75009 between 2018-12-20 and 2018-12-21\n",
      "- 75019 between 2018-11-08 and 2018-11-08\n",
      "- 75008 between 2018-11-09 and 2018-11-16\n",
      "- 75018 between 2018-12-07 and 2018-12-07\n",
      "- 75001 between 2018-11-28 and 2018-11-28\n",
      "- 75020 between 2018-11-29 and 2018-11-29\n",
      "- 75008 between 2018-11-16 and 2018-11-16\n",
      "- 75009 between 2018-11-27 and 2018-11-27\n",
      "- 75001 between 2018-11-28 and 2018-11-28\n",
      "- 75020 between 2018-12-20 and 2018-12-20\n",
      "- 75019 between 2018-11-20 and 2018-11-26\n",
      "- 75018 between 2018-12-10 and 2018-12-10\n",
      "- 75018 between 2018-12-11 and 2018-12-11\n",
      "- 75008 between 2018-11-14 and 2018-11-14\n",
      "- 75019 between 2018-11-22 and 2018-11-22\n",
      "- 75116 between 2018-12-06 and 2018-12-08\n",
      "- 75001 between 2018-11-28 and 2018-11-28\n",
      "- 75116 between 2018-12-06 and 2018-12-08\n",
      "- 75018 between 2018-12-13 and 2018-12-20\n",
      "- 75013 between 2018-12-03 and 2018-12-04\n",
      "- 75018 between 2018-12-10 and 2018-12-12\n",
      "- 75020 between 2018-11-30 and 2018-11-30\n",
      "- 75020 between 2018-11-30 and 2018-11-30\n",
      "- 75019 between 2018-12-05 and 2018-12-05\n",
      "- 75020 between 2018-11-29 and 2018-11-29\n",
      "- 75020 between 2018-11-29 and 2018-11-29\n",
      "- 75009 between 2018-12-20 and 2018-12-20\n",
      "- 75008 between 2018-11-09 and 2018-11-09\n",
      "- 75020 between 2018-12-20 and 2018-12-20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_movie_displays = [display_movie(m) for m in movies]\n",
    "print('\\n'.join(all_movie_displays[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188458f",
   "metadata": {},
   "source": [
    "- Display for each district its number of shootings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25c863",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbf057c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.806608Z",
     "start_time": "2024-10-11T07:03:35.793705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'75012': 596,\n",
       " '75020': 587,\n",
       " '75011': 641,\n",
       " '75019': 745,\n",
       " '75018': 1043,\n",
       " '75005': 640,\n",
       " '75009': 642,\n",
       " '75001': 722,\n",
       " '75004': 670,\n",
       " '75010': 749,\n",
       " '75007': 657,\n",
       " '75003': 236,\n",
       " '75017': 378,\n",
       " '75013': 658,\n",
       " '75008': 798,\n",
       " '75015': 363,\n",
       " '75002': 297,\n",
       " '75006': 471,\n",
       " '75116': 421,\n",
       " '75016': 614,\n",
       " '75014': 321,\n",
       " '94320': 4,\n",
       " '<arrondissement missing>': 1,\n",
       " '93500': 6,\n",
       " '93320': 1,\n",
       " '92220': 1,\n",
       " '92170': 1,\n",
       " '93200': 1,\n",
       " '93000': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def district_count_reducer(acc: Dict[str, int], movie: Movie) -> Dict[str, int]:\n",
    "  for shooting in movie['shootings']:\n",
    "    d = shooting['district']\n",
    "    if d not in acc:\n",
    "      acc[d] = 0\n",
    "    acc[d] += 1\n",
    "  return acc\n",
    "\n",
    "stats = functools.reduce(district_count_reducer, movies, dict())\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84df8db",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "This exercise asks to transform an array of `Movie`s to a hash mapping a piece of information within a `Movie` to an integer counting occurrences. This is a prime use-case for `reduce`, as we are changing the data type.\n",
    "\n",
    "We initialize a `reduce` call on `movies` with a function, and an initial value of an empty dictionary. The pieces of data we need to count for each `Movie` is in the `'shootings'` key, which is an array. Therefore we loop, and increment the accumulator key corresponding to the information we care about (`'district'`) for each shooting location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b58306",
   "metadata": {},
   "source": [
    "# Exercice 4 - Analyze CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2fe268",
   "metadata": {},
   "source": [
    "- Write a Python code retrieves the file of the most loaned titles in libraries in Paris at: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "663845ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.818501Z",
     "start_time": "2024-10-11T07:03:35.808870Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://opendata.paris.fr/explore/dataset/les-titres-les-plus-pretes/download/?format=csv&timezone=Europe/Berlin&lang=en&use_labels_for_header=true&csv_separator=%3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3296ee1",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9256ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get, Session\n",
    "from io import StringIO\n",
    "import csv\n",
    "s = Session()\n",
    "data = s.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ac1f0",
   "metadata": {},
   "source": [
    "### Explanlation\n",
    "\n",
    "This code retrieves CSV data from the provided URL using the `requests` library within a `session`, which handles persistent connections. After the data is fetched, it is stored as a csv string in the `data` variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d665083",
   "metadata": {},
   "source": [
    "- Analyze the resulting CSV file to display, for all entries: title, author, and total number of loans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c39b1",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22877be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.078350Z",
     "start_time": "2024-10-11T07:03:36.075015Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type de document;Prêts 2022;Titre;Auteur;Nombre de localisations;Nombre de prêt total;Nombre d'exemplaires\n",
      "Bande dessinée jeunesse;1064;Razzia;Sobral,  Patrick;47;2938;67\n",
      "Bande dessinée jeunesse;1024;Touche pas à mon veau;Guibert,  Emmanuel;45;2296;71\n",
      "Bande dessinée jeunesse;1016;Max et Lili vont chez papy et mamie;Saint-Mars,  Dominique de;50;5554;103\n",
      "Bande dessinée jeunesse;938;Lili veut un petit chat;Saint-Mars,  Dominique de;51;5789;80\n",
      "Bande dessinée jeunesse;921;Max et Lili font du camping;Saint-Mars,  Dominique de;52;5658;83\n",
      "Bande dessinée jeunesse;901;Lili trouve sa maîtresse méch\n",
      "total len of data: 66061\n",
      "total entries: 842\n"
     ]
    }
   ],
   "source": [
    "print(data[:600])\n",
    "print(f'total len of data: {len(data)}')\n",
    "\n",
    "books = [] # Save all retrieved data\n",
    "with StringIO(data) as csvfile:\n",
    "    r = csv.reader(csvfile, delimiter=';')\n",
    "    for i, row in enumerate(r):\n",
    "        if i == 0:\n",
    "            # ignore first row: column names\n",
    "            continue\n",
    "        book = {\n",
    "        \"Type\": row[0],\n",
    "        \"Loans\": int(row[1]),\n",
    "        \"Title\": row[2],\n",
    "        \"Author\": row[3],\n",
    "        \"Area\": row[4],\n",
    "        \"Total_Loans\": int(row[5]),\n",
    "        \"Total_Copies\": int(row[6])\n",
    "        }\n",
    "        books.append(book)\n",
    "\n",
    "print(f'total entries: {len(books)}')\n",
    "\n",
    "def disp_book(book: dict) -> str:\n",
    "    title = book['Title']\n",
    "    author = book['Author']\n",
    "    loans = book['Total_Loans']\n",
    "    return f'\"{title}\", by {author} ({loans} loans)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efad4fa",
   "metadata": {},
   "source": [
    "### Explanlation\n",
    "\n",
    "To inspect part of the data, the first 600 characters are printed along with the total length of the dataset. There are 7 columns shown in header: Type de document;Prêts 2022;Titre;Auteur;Nombre de localisations;Nombre de prêt total;Nombre d'exemplaires. \n",
    "\n",
    "Next, the CSV data is parsed using Python's csv.reader, and a list of dictionaries (`books`) is created to store the processed information. Each row (after the header) is treated as a separate book record. Since the delimiter is **';'** but not **','**, I searched the `csv.reader` method's documentation(https://docs.python.org/3/library/csv.html#csv-fmt-params) and found that `delimiter` parameter is used to specify customized delimiter.\n",
    "\n",
    "The first row, which contains the column headers, is skipped using `if i == 0`. Then, for each subsequent row, a dictionary is created with the following keys:\n",
    "\n",
    "- Type: Type de document\n",
    "- Loans: Prêts 2022 \n",
    "- Title: Titre\n",
    "- Author: Auteur\n",
    "- Area: Nombre de localisations\n",
    "- Total_Loans: Nombre de prêt total\n",
    "- Total_Copies: Nombre d'exemplaires\n",
    "  \n",
    "Each of these dictionaries is appended to the `books` list. Finally, the total number of books processed is printed using len(books).\n",
    "\n",
    "Since each entry corresponds to a dict element stored in the `books` list, displaying the title, author, and total number of loans is just looking up the relevant keys in each dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1b29eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.115744Z",
     "start_time": "2024-10-11T07:03:36.103623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Razzia\", by Sobral,  Patrick (2938 loans)\n",
      "\"Touche pas à mon veau\", by Guibert,  Emmanuel (2296 loans)\n",
      "\"Max et Lili vont chez papy et mamie\", by Saint-Mars,  Dominique de (5554 loans)\n",
      "\"Lili veut un petit chat\", by Saint-Mars,  Dominique de (5789 loans)\n",
      "\"Max et Lili font du camping\", by Saint-Mars,  Dominique de (5658 loans)\n",
      "\"Lili trouve sa maîtresse méchante\", by Saint-Mars,  Dominique de (4694 loans)\n",
      "\"J'irai où tu iras\", by Lyfoung,  Patricia (4707 loans)\n",
      "\"Les nerfs à vif\", by Nob (2837 loans)\n",
      "\"Je crois que je t'aime\", by Lyfoung,  Patricia (3878 loans)\n",
      "\"Attention tornade\", by Cazenove,  Christophe (2366 loans)\n",
      "\"Max et Lili se posent des questions sur Dieu\", by Saint-Mars,  Dominique de (4823 loans)\n",
      "\"Game over. 13. Toxic affair\", by Midam (2652 loans)\n",
      "\"Les Schtroumpfs et la tempête blanche\", by Jost,  Alain (975 loans)\n",
      "\"On a marché sur la lune\", by Hergé (5674 loans)\n",
      "\"Astérix chez les Bretons\", by Goscinny,  René (3014 loans)\n",
      "\"Parvati\", by Ogaki,  Philippe (2616 loans)\n",
      "\"Les Schtroumpfs et l'arbre d'or\", by Culliford,  Thierry (3460 loans)\n",
      "\"La décision : roman\", by Tuil,  Karine (976 loans)\n",
      "\"Les cahiers d'Esther. 4. Histoires de mes 13 ans\", by Sattouf,  Riad (2171 loans)\n",
      "\"Salut, les zinzins !\", by Cohen,  Jacqueline (4565 loans)\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join( [disp_book(b) for b in books[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1043a24",
   "metadata": {},
   "source": [
    "- Display for each type of document (there can be several entries for the same type of document), the total number of loans for this type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6077c6",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48b92fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.142096Z",
     "start_time": "2024-10-11T07:03:36.130485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bande dessinée jeunesse': 2300143,\n",
       " 'Livre adulte': 41731,\n",
       " 'Bande dessinée adulte': 59726,\n",
       " 'Livre sonore jeunesse': 10630,\n",
       " 'Livre jeunesse': 104067,\n",
       " 'Bande dessinée ado': 29819,\n",
       " 'DVD jeunesse': 2471,\n",
       " 'Jeux vidéos tous publics Non prêtables': 4235,\n",
       " 'Jeux de société prêtable': 10057,\n",
       " 'Musique jeunesse': 4792,\n",
       " 'Jeux de société': 1753}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = {}\n",
    "for b in books:\n",
    "    if b['Type'] not in stats:\n",
    "        stats[b['Type']] = b['Total_Loans']\n",
    "    else:\n",
    "        stats[b['Type']] += b['Total_Loans']\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ebc88",
   "metadata": {},
   "source": [
    "### Explanlation\n",
    "\n",
    "This code calculates the total number of loans for each document type. It loops through the `books` list and checks if the document type (`b['Type']`) is already in the `stats` dictionary. If not, it adds the type and sets the total loans to the current book’s `Total_Loans`. If the type is already in `stats`, it adds the current book's loans to the existing total. After the loop, `stats` contains the total number of loans for each document type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2853229",
   "metadata": {},
   "source": [
    "- Display titles in order of profitability (in descending order of the number of loans per copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26d95669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.167624Z",
     "start_time": "2024-10-11T07:03:36.158896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Console Nintendo Switch\" (1648 loans, 2 copies)\n",
      "\"Console PlayStation 4\" (2587 loans, 6 copies)\n",
      "\"SOS ouistiti :\" (1868 loans, 5 copies)\n",
      "\"Quatre en ligne :\" (1753 loans, 5 copies)\n",
      "\"Perplexus : : original\" (2254 loans, 8 copies)\n",
      "\"Un enfant chez les schtroumpfs\", by Díaz Vizoso,  Miguel (4504 loans, 43 copies)\n",
      "\"Mon meilleur ami\", by Verron,  Laurent (4662 loans, 47 copies)\n",
      "\"Les vacances infernales\", by Cohen,  Jacqueline (5014 loans, 51 copies)\n",
      "\"Bande de sauvages !\", by Cohen,  Jacqueline (5761 loans, 60 copies)\n",
      "\"Trop, c'est trop !\", by Cohen,  Jacqueline (4504 loans, 47 copies)\n",
      "\"Les fous du mercredi\", by Cohen,  Jacqueline (5169 loans, 54 copies)\n",
      "\"Ca va chauffer !\", by Cohen,  Jacqueline (4071 loans, 44 copies)\n",
      "\"Uno :\" (3136 loans, 34 copies)\n",
      "\"Ca roule !\", by Cohen,  Jacqueline (5763 loans, 63 copies)\n",
      "\"Salut, les zinzins !\", by Cohen,  Jacqueline (4565 loans, 50 copies)\n",
      "\"Les deux terreurs\", by Cohen,  Jacqueline (3999 loans, 44 copies)\n",
      "\"Subliiiimes !\", by Cohen,  Jacqueline (5007 loans, 56 copies)\n",
      "\"Un copieur sachant copier\", by Godi,  Bernard (3481 loans, 39 copies)\n",
      "\"A l'attaque !\", by Cohen,  Jacqueline (4353 loans, 49 copies)\n",
      "\"Tom-Tom et l'impossible Nana\", by Cohen,  Jacqueline (5832 loans, 66 copies)\n"
     ]
    }
   ],
   "source": [
    "def disp_book(book: dict) -> str:\n",
    "    title = book['Title']\n",
    "    author = book['Author']\n",
    "    loans = book['Total_Loans']\n",
    "    copies = book['Total_Copies']\n",
    "    if author:\n",
    "        return f'\"{title}\", by {author} ({loans} loans, {copies} copies)'\n",
    "    else:\n",
    "        return f'\"{title}\" ({loans} loans, {copies} copies)'\n",
    "\n",
    "\n",
    "for b in books:\n",
    "    b['Profitability'] = b['Total_Loans'] / b['Total_Copies']\n",
    "\n",
    "sorted_books = sorted(books, key=lambda x: x['Profitability'], reverse=True)\n",
    "print('\\n'.join( [disp_book(b) for b in sorted_books[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7016df",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "In this code, the `disp_book` function has been slightly modified to handle books that don't have an author. The function now displays the title, total loans, and total copies, and if the book has an author, it includes the author in the output. If the author is missing, it only displays the title, loans, and copies.\n",
    "\n",
    "The books are sorted by a new field called `Profitability`, which is calculated as the ratio of `Total_Loans` to `Total_Copies` for each book. The books with higher profitability are ranked higher.\n",
    "\n",
    "The `sorted` function, which was found through a search on Stack Overflow (https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value), is used to sort the `books` list in descending order of profitability. `key` parameter specifies a function to be called on each element before sorting. In this case, we use a lambda function: `lambda x: x['Profitability']`. This lambda function takes a book (x) and returns its Profitability value, which is used as the sorting criterion. Then `reverse=True` is used to sort the books in descending order, meaning the books with the highest profitability will appear first.\n",
    "\n",
    "Finally, the top 20 most profitable books are displayed using the `disp_book` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5668b",
   "metadata": {},
   "source": [
    "# Exercice 5 * - Analyze HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85237162",
   "metadata": {},
   "source": [
    "- Write a Python program that gets the content of the Wikipedia page at: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57dc6b89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.180211Z",
     "start_time": "2024-10-11T07:03:36.168632Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_density\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7e866",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67717ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-cu\n"
     ]
    }
   ],
   "source": [
    "s = Session()\n",
    "r = s.get(url)\n",
    "print(r.text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271cbfd",
   "metadata": {},
   "source": [
    "- Display all the countries mentioned in the table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ff5df",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f689c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "def _is_country(a_tags: list) -> bool:\n",
    "    \"\"\" \n",
    "    Filter some locations that are not country.\n",
    "    Case 1, some part of a country. Ex: Hong Kong (China)\n",
    "    Case 2, World\n",
    "    \"\"\"\n",
    "    is_country = True\n",
    "\n",
    "    # Case 1, check if the last <a> tag contains \")\" \n",
    "    if len(a_tags) > 1:\n",
    "        last_text = a_tags[len(a_tags)-1].next_sibling.strip()\n",
    "        # remove quotes and spaces\n",
    "        last_text_cleaned = last_text.strip('\"')\n",
    "        if last_text_cleaned == \")\" : \n",
    "            is_country = False\n",
    "    # Case 2, some row gives \"World\"\n",
    "    elif a_tags[0].text == 'World':\n",
    "        is_country = False\n",
    "    return is_country\n",
    "\n",
    "\n",
    "def find_country(s: Soup) -> list:\n",
    "    countries = []\n",
    "    for i, tr in enumerate(s.tbody.find_all('tr')):\n",
    "        if i == 0:\n",
    "            # Skip the first row with column names\n",
    "            continue\n",
    "        td = tr.find('td')\n",
    "        a_tags = td.find_all('a')\n",
    "        if _is_country(a_tags):\n",
    "            countries.append(a_tags[0].text)\n",
    "    return(countries)\n",
    "\n",
    "soup = Soup(r.text, features=\"html.parser\")\n",
    "country_table = soup.find('table')\n",
    "countries = find_country(country_table)\n",
    "print(len(countries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefd0fd",
   "metadata": {},
   "source": [
    "### Explanlation\n",
    "At first glance, this wiki page reveals that all the country information is stored in a table. To retrieve the entire table's content, the `soup.find('table')` method is utilized. Next, to inspect the HTML structure of the table, the Developer Tools in Chrome (F12) are employed.\n",
    "\n",
    "The first layer of the table structure is as follows:\n",
    "```\n",
    "<table>\n",
    "    <caption>\n",
    "    </caption>\n",
    "    <thead>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "    </tbody>\n",
    "</table>\n",
    "```\n",
    "In this structure, `<table>` encompasses all the content of the table, `<caption>` describes the table's content, `<thead>` defines the column names, and `<tbody>` contains the actual data rows. The `<tbody>` tag is the main focus for extracting country information from the table.\n",
    "\n",
    "Within the `<tbody>`, the content is organized into different row tags `<tr>`, each containing several table data elements `<td>`. The structure can be outlined as follows:\n",
    "```\n",
    "<tr>\n",
    "    <td> \n",
    "    locations(country)\n",
    "    </td>\n",
    "    <td> \n",
    "    Pop. /km2\n",
    "    </td>\n",
    "    <td>\n",
    "    Pop. /sq mi\n",
    "    </td>\n",
    "    <td>\n",
    "    Population\n",
    "    </td>\n",
    "    <td>\n",
    "    Area(km2)\n",
    "    </td>\n",
    "    <td>\n",
    "    Area(sq mi)\n",
    "    </td>\n",
    "    <td>\n",
    "    Note\n",
    "    </td>\n",
    "</tr>\n",
    "```\n",
    "Note that `<thread>` in Developer Tools does not exactly correspond to the structure of raw html. The header of the `<thread>` is actually the first `<tr>` tag inside the `<tbody>`, thid is why the first `<tr>` is skipped when dealing with reading the country information. The other `<td>` tag describes the region and country information. To illustrate this further, let’s examine the first line(which is the second `<tr>` tag) for Macau (China):\n",
    "```\n",
    "<td>\n",
    "    <span class=\"flagicon\"> Something about flag icon</span>\n",
    "    \"&nbsp\"\n",
    "    <a href=\"/wiki/Demographics_of_Macau\" title=\"Demographics of Macau\">Macau</a>\n",
    "    \" (\"\n",
    "    <a href=\"/wiki/China\" title=\"China\">China</a>\n",
    "    \")\"\n",
    "    <\\td>\n",
    "```\n",
    "\n",
    "Based on observations, the first `<a>` tag always contains the corresponding country information, so we store `a_tags[0].text` in the list. However, we note two additional cases to consider: \n",
    "\n",
    "1. **Dependencies**: The table also includes information about dependencies along with countries, so we need to remove these rows. We can identify these rows by noting that any cell containing a dependency has a corresponding country in parentheses. Therefore, we check for cases where there is more than one `<a>` tag and determine if the next word after the last `<a>` tag is a closing parenthesis `\")\"` to decide if it is a dependency. To find the next word, I consulted ChatGPT, which informed me that I can use the `next_sibling` method. A sibling refers to an element that shares the same parent. The next sibling node of the specified element, which can be a tag, text node, or comment. In this case it is a text node `\")\"`.\n",
    "\n",
    "2. **The term \"World\"**: Some rows display the term \"World\" (for example, \"World (excluding Antarctica)\"). In this case, we simply check if the `a_tags[0].text` corresponding to the country name is the string `\"World\"` to exclude it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff7ad3f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.660044Z",
     "start_time": "2024-10-11T07:03:36.649884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monaco',\n",
       " 'Singapore',\n",
       " 'Bahrain',\n",
       " 'Maldives',\n",
       " 'Malta',\n",
       " 'Vatican City',\n",
       " 'Bangladesh',\n",
       " 'Taiwan',\n",
       " 'Mauritius',\n",
       " 'Barbados',\n",
       " 'Nauru',\n",
       " 'San Marino',\n",
       " 'Rwanda',\n",
       " 'South Korea',\n",
       " 'Lebanon',\n",
       " 'Burundi',\n",
       " 'Tuvalu',\n",
       " 'India',\n",
       " 'Netherlands',\n",
       " 'Haiti',\n",
       " 'Israel',\n",
       " 'Philippines',\n",
       " 'Belgium',\n",
       " 'Comoros',\n",
       " 'Grenada',\n",
       " 'Sri Lanka',\n",
       " 'Japan',\n",
       " 'El Salvador',\n",
       " 'Pakistan',\n",
       " 'Trinidad and Tobago',\n",
       " 'Vietnam',\n",
       " 'Saint Lucia',\n",
       " 'United Kingdom',\n",
       " 'Saint Vincent and the Grenadines',\n",
       " 'Jamaica',\n",
       " 'Luxembourg',\n",
       " 'Liechtenstein',\n",
       " 'Gambia',\n",
       " 'Nigeria',\n",
       " 'Kuwait',\n",
       " 'São Tomé and Príncipe',\n",
       " 'Seychelles',\n",
       " 'Qatar',\n",
       " 'Germany',\n",
       " 'Dominican Republic',\n",
       " 'Marshall Islands',\n",
       " 'Malawi',\n",
       " 'North Korea',\n",
       " 'Antigua and Barbuda',\n",
       " 'Switzerland',\n",
       " 'Nepal',\n",
       " 'Uganda',\n",
       " 'Italy',\n",
       " 'Kiribati',\n",
       " 'Saint Kitts and Nevis',\n",
       " 'Andorra',\n",
       " 'Guatemala',\n",
       " 'Micronesia',\n",
       " 'Togo',\n",
       " 'Kosovo',\n",
       " 'China',\n",
       " 'Cape Verde',\n",
       " 'Isle of Man',\n",
       " 'Indonesia',\n",
       " 'Tonga',\n",
       " 'Ghana',\n",
       " 'Thailand',\n",
       " 'Denmark',\n",
       " 'Cyprus',\n",
       " 'United Arab Emirates',\n",
       " 'Transnistria',\n",
       " 'Czech Republic',\n",
       " 'Jordan',\n",
       " 'Syria',\n",
       " 'Sierra Leone',\n",
       " 'Poland',\n",
       " 'Azerbaijan',\n",
       " 'Benin',\n",
       " 'Slovakia',\n",
       " 'Ethiopia',\n",
       " 'Northern Cyprus',\n",
       " 'Egypt',\n",
       " 'Portugal',\n",
       " 'Turkey',\n",
       " 'Hungary',\n",
       " 'Austria',\n",
       " 'Iraq',\n",
       " 'Slovenia',\n",
       " 'Malaysia',\n",
       " 'Costa Rica',\n",
       " 'Cuba',\n",
       " 'Moldova',\n",
       " 'Albania',\n",
       " 'Dominica',\n",
       " 'Spain',\n",
       " 'Honduras',\n",
       " 'Cambodia',\n",
       " 'Armenia',\n",
       " 'Kenya',\n",
       " 'East Timor',\n",
       " 'Senegal',\n",
       " 'Ivory Coast',\n",
       " 'Burkina Faso',\n",
       " 'Romania',\n",
       " 'North Macedonia',\n",
       " 'Serbia',\n",
       " 'Myanmar',\n",
       " 'Samoa',\n",
       " 'Brunei',\n",
       " 'Greece',\n",
       " 'Uzbekistan',\n",
       " 'Lesotho',\n",
       " 'Tunisia',\n",
       " 'Ireland',\n",
       " 'Cook Islands',\n",
       " 'Tajikistan',\n",
       " 'Tanzania',\n",
       " 'Croatia',\n",
       " 'Ecuador',\n",
       " 'Eswatini',\n",
       " 'Mexico',\n",
       " 'Yemen',\n",
       " 'Afghanistan',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Equatorial Guinea',\n",
       " 'Ukraine',\n",
       " 'Bulgaria',\n",
       " 'Cameroon',\n",
       " 'Guinea-Bissau',\n",
       " 'Panama',\n",
       " 'Guinea',\n",
       " 'Iran',\n",
       " 'Nicaragua',\n",
       " 'Georgia',\n",
       " 'Morocco',\n",
       " 'Madagascar',\n",
       " 'Fiji',\n",
       " 'South Africa',\n",
       " 'Djibouti',\n",
       " 'Liberia',\n",
       " 'Easter Island',\n",
       " 'Belarus',\n",
       " 'Colombia',\n",
       " 'Montenegro',\n",
       " 'DR Congo',\n",
       " 'Zimbabwe',\n",
       " 'Mozambique',\n",
       " 'Lithuania',\n",
       " 'Palau',\n",
       " 'United States',\n",
       " 'Kyrgyzstan',\n",
       " 'Laos',\n",
       " 'Venezuela',\n",
       " 'Eritrea',\n",
       " 'Bahamas',\n",
       " 'Angola',\n",
       " 'Somaliland',\n",
       " 'Estonia',\n",
       " 'Somalia',\n",
       " 'Latvia',\n",
       " 'Abkhazia',\n",
       " 'Vanuatu',\n",
       " 'Zambia',\n",
       " 'Sudan',\n",
       " 'Peru',\n",
       " 'Chile',\n",
       " 'Solomon Islands',\n",
       " 'Brazil',\n",
       " 'Sweden',\n",
       " 'Papua New Guinea',\n",
       " 'Niger',\n",
       " 'Bhutan',\n",
       " 'Uruguay',\n",
       " 'New Zealand',\n",
       " 'Algeria',\n",
       " 'Mali',\n",
       " 'Belize',\n",
       " 'Congo',\n",
       " 'South Sudan',\n",
       " 'Saudi Arabia',\n",
       " 'Finland',\n",
       " 'Argentina',\n",
       " 'South Ossetia',\n",
       " 'Paraguay',\n",
       " 'Oman',\n",
       " 'Chad',\n",
       " 'Norway',\n",
       " 'Turkmenistan',\n",
       " 'Bolivia',\n",
       " 'Central African Republic',\n",
       " 'Gabon',\n",
       " 'Russia',\n",
       " 'Niue',\n",
       " 'Kazakhstan',\n",
       " 'Mauritania',\n",
       " 'Botswana',\n",
       " 'Libya',\n",
       " 'Canada',\n",
       " 'Suriname',\n",
       " 'Guyana',\n",
       " 'Iceland',\n",
       " 'Australia',\n",
       " 'Namibia',\n",
       " 'Mongolia']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32678327",
   "metadata": {},
   "source": [
    "- Display for each country its rank, density, population, area. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a9623",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85f9435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "{'rank': 3, 'density': '1,910 pop/Km2', 'population': '1,485,510', 'area': '778 Km2'}\n"
     ]
    }
   ],
   "source": [
    "def show_save_country(s: Soup) -> dict:\n",
    "    \"\"\"Parse the table and save country data.\"\"\"\n",
    "    countries = {}\n",
    "    rank = 1\n",
    "    country = None  \n",
    "\n",
    "    for i, tr in enumerate(s.tbody.find_all('tr')):\n",
    "        if i == 0:\n",
    "            # Skip the header row\n",
    "            continue\n",
    "        for j, td in enumerate(tr.find_all('td')):\n",
    "            # Handle country name (column 0)\n",
    "            if j == 0:\n",
    "                a_tags = td.find_all('a')\n",
    "                if _is_country(a_tags):\n",
    "                    country = a_tags[0].text.strip()\n",
    "                    countries[country] = {}\n",
    "                    countries[country]['rank'] = rank\n",
    "                    rank += 1\n",
    "                else:\n",
    "                    country = None  \n",
    "\n",
    "            # Handle density (column 1)\n",
    "            elif j == 1 and country:\n",
    "                density = td.text.strip() + \" pop/Km2\"\n",
    "                countries[country]['density'] = density\n",
    "\n",
    "            # Handle population (column 3)\n",
    "            elif j == 3 and country:\n",
    "                pop = td.text \n",
    "                countries[country]['population'] = pop\n",
    "\n",
    "            # Handle area (column 4)\n",
    "            elif j == 4 and country:\n",
    "                area = td.text.strip() + \" Km2\"\n",
    "                countries[country]['area'] = area\n",
    "\n",
    "    return countries\n",
    "\n",
    "soup = Soup(r.text, features=\"html.parser\")\n",
    "country_dict = show_save_country(soup)\n",
    "print(len(country_dict))\n",
    "print(country_dict['Bahrain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7e818",
   "metadata": {},
   "source": [
    "### Explanlation\n",
    "As explained earlier for the `<table>` structure, the information we need is basically in the different `<td>` cells corresponding to each `<tr>` row, and we just need to use `tr.find_all('td')` to find the corresponding information. We only recorded density and area in Km2. When calculating the rankings, we iteratively increment the rankings sequentially starting from the table header, and only increment the rankings and record information if the `<a>` tag is decided to be a country. To illustrate, we display the information of country 'Bahrain'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3525d8",
   "metadata": {},
   "source": [
    "- Save the information obtained in a Python dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b00cbf",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c31344",
   "metadata": {},
   "source": [
    "- Using the previously saved Python dictionary, ask the user for a country, display the \n",
    "corresponding information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d58996",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc69cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting country_interact.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile country_interact.py\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from requests import get, Session\n",
    "\n",
    "def _is_country(a_tags: list) -> bool:\n",
    "    \"\"\" \n",
    "    Filter some locations that are not country.\n",
    "    Case 1, some part of a country. Ex: Hong Kong (China)\n",
    "    Case 2, World\n",
    "    \"\"\"\n",
    "    is_country = True\n",
    "\n",
    "    # Case 1, check if the last <a> tag contains \")\" \n",
    "    if len(a_tags) > 1:\n",
    "        last_text = a_tags[len(a_tags)-1].next_sibling.strip()\n",
    "        # remove quotes and spaces\n",
    "        last_text_cleaned = last_text.strip('\"')\n",
    "        if last_text_cleaned == \")\" : \n",
    "            is_country = False\n",
    "    # Case 2, some row gives \"World\"\n",
    "    elif a_tags[0].text == 'World':\n",
    "        is_country = False\n",
    "    return is_country\n",
    "\n",
    "def show_save_country(s: Soup) -> dict:\n",
    "    \"\"\"Parse the table and save country data.\"\"\"\n",
    "    countries = {}\n",
    "    rank = 1\n",
    "    country = None  \n",
    "\n",
    "    for i, tr in enumerate(s.tbody.find_all('tr')):\n",
    "        if i == 0:\n",
    "            # Skip the header row\n",
    "            continue\n",
    "        for j, td in enumerate(tr.find_all('td')):\n",
    "            # Handle country name (column 0)\n",
    "            if j == 0:\n",
    "                a_tags = td.find_all('a')\n",
    "                if _is_country(a_tags):\n",
    "                    country = a_tags[0].text.strip()\n",
    "                    countries[country] = {}\n",
    "                    countries[country]['rank'] = rank\n",
    "                    rank += 1\n",
    "                else:\n",
    "                    country = None  \n",
    "\n",
    "            # Handle density (column 1)\n",
    "            elif j == 1 and country:\n",
    "                density = td.text.strip() + \" pop/Km2\"\n",
    "                countries[country]['density'] = density\n",
    "\n",
    "            # Handle population (column 3)\n",
    "            elif j == 3 and country:\n",
    "                pop = td.text \n",
    "                countries[country]['population'] = pop\n",
    "\n",
    "            # Handle area (column 4)\n",
    "            elif j == 4 and country:\n",
    "                area = td.text.strip() + \" Km2\"\n",
    "                countries[country]['area'] = area\n",
    "\n",
    "    return countries\n",
    "\n",
    "def get_country_info(countries: dict) -> None:\n",
    "    while True:\n",
    "        country_name = input(\"\\nPlease enter a country name (or 'q' to quit): \").strip()\n",
    "        if country_name.lower() == 'q':\n",
    "            print(\"Exiting the program. Goodbye!\")\n",
    "            break\n",
    "        if country_name in countries:\n",
    "            info = countries[country_name]\n",
    "            print(f\"\\nInformation for {country_name}:\")\n",
    "            print(f\"Rank: {info['rank']}\")\n",
    "            print(f\"Density: {info['density']}\")\n",
    "            print(f\"Population: {info['population']}\")\n",
    "            print(f\"Area: {info['area']}\")\n",
    "        else:\n",
    "            print(f\"Sorry, information for {country_name} is not available.\")\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_density\"\n",
    "s = Session()\n",
    "r = s.get(url)\n",
    "soup = Soup(r.text, features=\"html.parser\")\n",
    "country_table = soup.find('table')\n",
    "country_dict = show_save_country(soup)\n",
    "get_country_info(country_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2971a9d",
   "metadata": {},
   "source": [
    "### Explanlation\n",
    "To enable users to interactively query country information, we have integrated the functions from this exercise that extract HTML data, retrieve country information, and store it in a dictionary. Additionally, we utilized the IPython magic commands mentioned in the slides to save the code from the cell locally, allowing us to run the script for interactive execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python country_interact.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de123f",
   "metadata": {},
   "source": [
    "# Exercice 6 * - API Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62855a4",
   "metadata": {},
   "source": [
    "- Write a Python program that will make available a Web API allowing elementary calculations on \n",
    "integers.\n",
    "\n",
    "The APIs are accessible by GET and in the form: \n",
    "- /add/{integer1}/{integer2}: add integer1 and integer2\n",
    "- /sub/{integer1}/{integer2}: perform the subtraction of integer1 and integer2\n",
    "- /mul/{integer1}/{integer2}: carry out the multiplication of integer1 and integer2\n",
    "- /div/{integer1}/{integer2}: perform the integer division of integer1 by integer2\n",
    "- /mod/{integer1}/{integer2}: perform the remainder of the integer division of integer1\n",
    "by integer2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad95976",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bcc6b6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:46.583700Z",
     "start_time": "2024-10-11T07:03:36.825290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.py\n",
    "import json\n",
    "from bottle import route, run, response\n",
    "from typing import Callable\n",
    "\n",
    "def define_operation_route(name: str, f: Callable[[int, int], int]) -> None:\n",
    "  @route(f'/{name}/<i1>/<i2>')\n",
    "  def r(i1, i2):\n",
    "    response.content_type = 'application/json; charset=utf-8'\n",
    "    result = f(int(i1), int(i2))\n",
    "    return json.dumps(result)\n",
    "\n",
    "define_operation_route('add', lambda x, y: x + y)\n",
    "define_operation_route('sub', lambda x, y: x - y)\n",
    "define_operation_route('mul', lambda x, y: x * y)\n",
    "define_operation_route('div', lambda x, y: x // y)\n",
    "define_operation_route('mod', lambda x, y: x % y)\n",
    "\n",
    "run(host='localhost', port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "efcf3d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://localhost:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3aae71",
   "metadata": {},
   "source": [
    "http://localhost:8080/mul/6/7\n",
    "\n",
    "http://localhost:8080/div/42/8\n",
    "\n",
    "http://localhost:8080/mod/42/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba86e6",
   "metadata": {},
   "source": [
    "- Write a Python program that will test the web API made available through the requests\n",
    "library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9fd5f",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7872ebdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_math.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_math.py\n",
    "import requests\n",
    "\n",
    "def request_and_assert(op, arg1, arg2, expected_result) -> None:\n",
    "  r = requests.get(f'http://localhost:8080/{op}/{arg1}/{arg2}')\n",
    "  assert r.status_code == 200\n",
    "  assert r.headers['Content-Type'] == 'application/json; charset=utf-8'\n",
    "  assert r.encoding == 'utf-8'\n",
    "  assert r.json() == expected_result\n",
    "\n",
    "def test_add():\n",
    "  request_and_assert('add', 15, 16, 31)\n",
    "  request_and_assert('add', -6, 12, 6)\n",
    "\n",
    "def test_sub():\n",
    "  request_and_assert('sub', 99, 44, 55)\n",
    "  request_and_assert('sub', 44, 55, -11)\n",
    "\n",
    "def test_mul():\n",
    "  request_and_assert('mul', 6, 7, 42)\n",
    "  request_and_assert('mul', 6, -7, -42)\n",
    "\n",
    "def test_div():\n",
    "  request_and_assert('div', 42, 8, 5)\n",
    "  request_and_assert('div', -17, 3, -6)\n",
    "\n",
    "def test_mod():\n",
    "  request_and_assert('mod', 42, 8, 2)\n",
    "  request_and_assert('mod', -17, 3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e4c81a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.2, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: /Users/graham.preston/fac_src/PROGRES/TME2\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "test_math.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.09s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e73e7a",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "We want to test the following things about each route:\n",
    "- that a success response is received\n",
    "- that the operation works\n",
    "- that the content type and encoding is correct\n",
    "\n",
    "Because this is repetitive for each route, a `request_and_assert` function is defined, rendering each test case to a single line of code. From here, two test cases are made per operation, for safety."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
